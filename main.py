import pandas as pd
import numpy as np
from hmmlearn import hmm
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import talib
from datetime import datetime
from xgboost import XGBClassifier
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from xgboost import plot_importance
from sklearn.model_selection import TimeSeriesSplit
import pickle
import requests
from requests.exceptions import ConnectionError, Timeout, TooManyRedirects
import json
import time
import urllib.parse
import hashlib
import hmac
import datetime
import pywt
import tkinter as tk
from tkinter import messagebox

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

api_key = 'IbIgJihiEgl4rEjWnOFazg7F4YVzJXVG8if3iKcGsurgspgblDN2F73XMPdUzOcH'

def load_and_preprocess_data(filepath):
    print(f"Loading data from {filepath}...")
    df = pd.read_csv(filepath, names=['Index', 'Timestamp', 'Open', 'High', 'Low', 'Close', 'Volume', 'Label'])
    df.drop(['Index'], axis=1, inplace=True)
    df.drop(0, inplace=True)
    #Drop useless row
    df = df.astype(float)

    eth_df = pd.read_csv('ETHUSDC15m_2020-2025.csv', names=['Timestamp', 'High', 'Low', 'Close'])
    eth_df.drop(0, inplace=True)
    eth_df = eth_df.astype(float)
    df['Label'] = (df['Close'].shift(-1) > df['Close']).astype(int)

    halving_dates = [
        datetime.datetime(2020, 5, 11),
        datetime.datetime(2024, 4, 20),
        datetime.datetime(2028, 2, 19) #next estimated halving
    ]
    halving_cycle_duration = datetime.timedelta(days=365)

    quarters = []
    months = []
    days = []
    hours = []
    sessions = []
    in_halving_cycle = []

    for timestamp in df['Timestamp']:
        utc_time = datetime.datetime.utcfromtimestamp(int(timestamp) / 1000)
        month = utc_time.month
        quarter = (month - 1) // 3 + 1
        day = utc_time.weekday()
        hour = utc_time.hour

        if 0 <= hour < 9:
            session = 0
        elif 7 <= hour < 16:
            session = 1
        elif 12 <= hour < 21:
            session = 2
        else:
            session = 0

        in_cycle = False
        for halving_date in halving_dates:
            if halving_date <= utc_time <= halving_date + halving_cycle_duration:
                in_cycle = True
                break

        quarters.append(quarter)
        months.append(month)
        days.append(day)
        hours.append(hour)
        sessions.append(session)
        in_halving_cycle.append(in_cycle)

    df['Quarter'] = quarters
    df['Month'] = months
    df['Day'] = days
    df['Hour'] = hours
    df['Session'] = sessions
    df['In_Halving_Cycle'] = in_halving_cycle
    df['In_Halving_Cycle'] = df['In_Halving_Cycle'].astype(int)

    #Intramarket Difference
    period = 24
    btc_cmma = cmma(df['High'], df['Low'], df['Close'], period)
    eth_cmma = cmma(eth_df['High'], eth_df['Low'], eth_df['Close'], period)
    df['BTC-ETC_Diff'] = btc_cmma - eth_cmma

    df['Price_Diff'] = df['Close'].diff()
    df["Returns"] = df["Close"].pct_change()
    df["Volatility"] = df["Returns"].rolling(window=24).std()
    df['Price_Diff_Future'] = df['Price_Diff'].shift(-1)
    df['Price_Diff_Future_Abs'] = df['Price_Diff_Future'].abs()

    #prob(buy) - prob(sell) = prob(total)
    #if prob(total) is positive that means the buy signal is stronger, if negative that means the sell signal is stronger
    #prob(total) * price_diff

    #Replaces 0's with a number close to 0 to avoid infinity being present in Volume_Change
    #Since the label column has 0's present, we need to make sure that they are not replaced
    df['Volume'] = df['Volume'].replace(0, 0.00000000000000001)
    df["Volume_Change"] = df["Volume"].pct_change()

    # Need this because there was a weird error were the data in these columns were not classified as floats, this caused a problem with the pipeline as I'm not using a target encoder
    df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce')
    df['Returns'] = pd.to_numeric(df['Returns'], errors='coerce')

    df['OBV'] = talib.OBV(df['Close'], df['Volume'])

    df['MFV'] = (((df['Close'] - df['Low']) - (df['High'] - df['Close']) / (df['High'] - df['Low'])) * df['Volume'])
    df['MFV'] = df['MFV'].fillna(0) #need this incase nans are generated by dividing by 0
    df['A/D'] = 0
    for i in range(2, len(df)):
        currentMFV = df.loc[i, 'MFV']
        df.loc[i, 'A/D'] = df.loc[i-1, 'A/D'] + currentMFV
    df['CMF'] = df['MFV'].rolling(window=21).sum() / df['Volume'].rolling(window=21).sum()

    df['CumulativeVolume'] = 0
    for i in range(2, len(df)):
        new_volume = df.loc[i-1, 'CumulativeVolume'] + df.loc[i, 'Volume']
        df.loc[i, 'CumulativeVolume'] = new_volume
        if i % 97 == 0: #reset the cumulative volume everyday
            df.loc[i, 'CumulativeVolume'] = df.loc[i, 'Volume']

    df['VWAP'] = (((df['High'] + df['Low'] + df['Close']) / 3) * df['Volume']) / df['CumulativeVolume']

    df['LOW_EMA'] = talib.EMA(df['Close'], timeperiod=9)
    df['HIGH_EMA'] = talib.EMA(df['Close'], timeperiod=21)
    df['RSI'] = talib.RSI(df['Close'], timeperiod=14)
    df['Aroon'] = talib.AROONOSC(df['High'], df['Low'], timeperiod=14)
    df['Fast_%K'], df['Fast_%D'] = talib.STOCHF(df['High'], df['Low'], df['Close'], fastk_period=14)
    df['WILLR'] = talib.WILLR(df['High'], df['Low'], df['Close'])
    df['CCI'] = talib.CCI(df['High'], df['Low'], df['Close'])
    df['ATR'] = talib.ATR(df['High'], df['Low'], df['Close'])
    df['ADX'] = talib.ADX(df['High'], df['Low'], df['Close'])

    df['Momentum'] = df['Close'].diff()
    df['Absolute_Momentum'] = df['Momentum'].abs()
    df['Short_EMA_Momentum'] = talib.EMA(df['Momentum'], timeperiod=13)
    df['Short_EMA_Absolute'] = talib.EMA(df['Absolute_Momentum'], timeperiod=13)
    df['Double_EMA_Momentum'] = talib.EMA(df['Short_EMA_Momentum'], timeperiod=25)
    df['Double_EMA_Absolute'] = talib.EMA(df['Short_EMA_Absolute'], timeperiod=25)
    df['TSI'] = 100 * (df['Double_EMA_Momentum'] / df['Double_EMA_Absolute'])

    df['BB_Upper'], df['BB_Middle'], df['BB_Lower'] = talib.BBANDS(df['Close'], 20)
    df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']

    macd, macdsignal, macdhist = talib.MACDFIX(df['Close'])
    df['MACD'] = macd
    df['MACDSignal'] = macdsignal
    df['MACDHist'] = macdhist

    df['PP'] = (df['High'].shift(1) + df['Low'].shift(1) + df['Close'].shift(1)) / 3
    df['R1'] = 2 * df['PP'] - df['Low'].shift(1)
    df['R2'] = df['PP'] + (df['High'].shift(1) - df['Low'].shift(1))
    df['S1'] = 2 * df['PP'] - df['High'].shift(1)
    df['S2'] = df['PP'] - (df['High'].shift(1) - df['Low'].shift(1))

    #Ichimoku Cloud
    df['Tenkan_Sen'] = (df['High'].rolling(window=9).max() + df['Low'].rolling(window=9).min()) / 2
    df['Kijun_Sen'] = (df['High'].rolling(window=26).max() + df['Low'].rolling(window=26).min()) / 2
    df['Senkou_Span_A'] = (df['Tenkan_Sen'] + df['Kijun_Sen']) / 2
    df['Senkou_Span_B'] = (df['High'].rolling(window=52).max() + df['Low'].rolling(window=52).min()) / 2
    df['Chikou_Span'] = df['Close'].shift(-26)

    #Keltner Channels
    df['KC_Middle'] = talib.EMA(df['Close'], timeperiod=20)
    df['KC_Upper'] = df['KC_Middle'] + df['ATR'] * 2
    df['KC_Lower'] = df['KC_Middle'] - df['ATR'] * 2
    df['KC_Width'] = df['KC_Upper'] - df['KC_Lower']

    #max/min points
    close = df['Close'].values
    length = len(close)
    slope = (talib.EMA(close, timeperiod=50) - talib.EMA(close, timeperiod=10)).tolist()
    extrema_unconfirmed = [0] * length

    for i in range(1, length - 1):
        if slope[i] < 0 and slope[i - 1] >= 0:
            extrema_unconfirmed[i] = -1
        if slope[i] > 0 and slope[i - 1] <= 0:
            extrema_unconfirmed[i] = 1

    df['Extrema_Unconfirmed'] = extrema_unconfirmed

    df['Confirmed_Fractal_Peak'] = (
            (df['Close'] > df['Close'].shift(1)) &
            (df['Close'] > df['Close'].shift(2)) &
            (df['Close'] > df['Close'].shift(-1)) &
            (df['Close'] > df['Close'].shift(-2))
    )
    df['Confirmed_Fractal_Trough'] = (
            (df['Close'] < df['Close'].shift(1)) &
            (df['Close'] < df['Close'].shift(2)) &
            (df['Close'] < df['Close'].shift(-1)) &
            (df['Close'] < df['Close'].shift(-2))
    )
    df['Confirmed_Fractal_Peak'] = df['Confirmed_Fractal_Peak'].astype(int)
    df['Confirmed_Fractal_Trough'] = df['Confirmed_Fractal_Trough'].astype(int)
    df['Confirmed_Fractal_Peak'] = df['Confirmed_Fractal_Peak'].shift(2)
    df['Confirmed_Fractal_Trough'] = df['Confirmed_Fractal_Trough'].shift(2)

    #Candlestick patterns
    df['Body_Size'] = abs(df['Close'] - df['Open'])
    df['Upper_Wick'] = df['High'] - df[['Open', 'Close']].max(axis=1)
    df['Lower_Wick'] = df[['Open', 'Close']].min(axis=1) - df['Low']
    df['Range'] = df['High'] - df['Low']

    #Add small number to avoid division by zero
    df['Body_Ratio'] = df['Body_Size'] / (df['Range'] + 1e-9)
    df['Upper_Wick_Ratio'] = df['Upper_Wick'] / (df['Range'] + 1e-9)
    df['Lower_Wick_Ratio'] = df['Lower_Wick'] / (df['Range'] + 1e-9)

    df['Two_Crows'] = talib.CDL2CROWS(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Crows'] = talib.CDL3BLACKCROWS(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Inside'] = talib.CDL3INSIDE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Strike'] = talib.CDL3LINESTRIKE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Outside'] = talib.CDL3OUTSIDE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Stars_South'] = talib.CDL3STARSINSOUTH(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Soldiers'] = talib.CDL3WHITESOLDIERS(df['Open'], df['High'], df['Low'], df['Close'])
    df['Abandoned_Baby'] = talib.CDLABANDONEDBABY(df['Open'], df['High'], df['Low'], df['Close'])
    df['Advance_Block'] = talib.CDLADVANCEBLOCK(df['Open'], df['High'], df['Low'], df['Close'])
    df['Belt_Hold'] = talib.CDLBELTHOLD(df['Open'], df['High'], df['Low'], df['Close'])
    df['Breakaway'] = talib.CDLBREAKAWAY(df['Open'], df['High'], df['Low'], df['Close'])
    df['Closing_Marubozu'] = talib.CDLCLOSINGMARUBOZU(df['Open'], df['High'], df['Low'], df['Close'])
    df['Baby_Swallow'] = talib.CDLCONCEALBABYSWALL(df['Open'], df['High'], df['Low'], df['Close'])
    df['Counterattack'] = talib.CDLCOUNTERATTACK(df['Open'], df['High'], df['Low'], df['Close'])
    df['Dark_Cloud'] = talib.CDLDARKCLOUDCOVER(df['Open'], df['High'], df['Low'], df['Close'])
    df['Doji'] = talib.CDLDOJI(df['Open'], df['High'], df['Low'], df['Close'])
    df['Doji_Star'] = talib.CDLDOJISTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['Dragonfly_Doji'] = talib.CDLDRAGONFLYDOJI(df['Open'], df['High'], df['Low'], df['Close'])
    df['Engulfing'] = talib.CDLENGULFING(df['Open'], df['High'], df['Low'], df['Close'])
    df['Evening_Doji'] = talib.CDLEVENINGDOJISTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['Evening_Star'] = talib.CDLEVENINGSTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['Up_Down_Gap'] = talib.CDLGAPSIDESIDEWHITE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Gravestone_Doji'] = talib.CDLGRAVESTONEDOJI(df['Open'], df['High'], df['Low'], df['Close'])
    df['Hammer'] = talib.CDLHAMMER(df['Open'], df['High'], df['Low'], df['Close'])
    df['Hanging_Man'] = talib.CDLHANGINGMAN(df['Open'], df['High'], df['Low'], df['Close'])
    df['Harami'] = talib.CDLHARAMI(df['Open'], df['High'], df['Low'], df['Close'])
    df['Harami_Cross'] = talib.CDLHARAMICROSS(df['Open'], df['High'], df['Low'], df['Close'])
    df['High_Wave'] = talib.CDLHIGHWAVE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Hikkake'] = talib.CDLHIKKAKE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Modified_Hikkake'] = talib.CDLHIKKAKEMOD(df['Open'], df['High'], df['Low'], df['Close'])
    df['Homing_Pigeon'] = talib.CDLHOMINGPIGEON(df['Open'], df['High'], df['Low'], df['Close'])
    df['Identical_Crows'] = talib.CDLIDENTICAL3CROWS(df['Open'], df['High'], df['Low'], df['Close'])
    df['In_Neck'] = talib.CDLINNECK(df['Open'], df['High'], df['Low'], df['Close'])
    df['Inverted_Hammer'] = talib.CDLINVERTEDHAMMER(df['Open'], df['High'], df['Low'], df['Close'])
    df['Kicking'] = talib.CDLKICKING(df['Open'], df['High'], df['Low'], df['Close'])
    df['Kicking_Length'] = talib.CDLKICKINGBYLENGTH(df['Open'], df['High'], df['Low'], df['Close'])
    df['Ladder_Bottom'] = talib.CDLLADDERBOTTOM(df['Open'], df['High'], df['Low'], df['Close'])
    df['Long_Doji'] = talib.CDLLONGLEGGEDDOJI(df['Open'], df['High'], df['Low'], df['Close'])
    df['Long_Candle'] = talib.CDLLONGLINE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Marubozu'] = talib.CDLMARUBOZU(df['Open'], df['High'], df['Low'], df['Close'])
    df['Matching_Low'] = talib.CDLMATCHINGLOW(df['Open'], df['High'], df['Low'], df['Close'])
    df['Mat_Hold'] = talib.CDLMATHOLD(df['Open'], df['High'], df['Low'], df['Close'])
    df['Morning_Doji'] = talib.CDLMORNINGDOJISTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['Morning_Star'] = talib.CDLMORNINGSTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['On_Neck'] = talib.CDLONNECK(df['Open'], df['High'], df['Low'], df['Close'])
    df['Piercing'] = talib.CDLPIERCING(df['Open'], df['High'], df['Low'], df['Close'])
    df['Rickshaw_Man'] = talib.CDLRICKSHAWMAN(df['Open'], df['High'], df['Low'], df['Close'])
    df['Rising_Falling'] = talib.CDLRISEFALL3METHODS(df['Open'], df['High'], df['Low'], df['Close'])
    df['Separating_Lines'] = talib.CDLSEPARATINGLINES(df['Open'], df['High'], df['Low'], df['Close'])
    df['Shooting_Star'] = talib.CDLSHOOTINGSTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['Short_Candle'] = talib.CDLSHORTLINE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Spinning_Top'] = talib.CDLSPINNINGTOP(df['Open'], df['High'], df['Low'], df['Close'])
    df['Stalled'] = talib.CDLSTALLEDPATTERN(df['Open'], df['High'], df['Low'], df['Close'])
    df['Stick_Sandwich'] = talib.CDLSTICKSANDWICH(df['Open'], df['High'], df['Low'], df['Close'])
    df['Takuri'] = talib.CDLTAKURI(df['Open'], df['High'], df['Low'], df['Close'])
    df['Tasuki_Gap'] = talib.CDLTASUKIGAP(df['Open'], df['High'], df['Low'], df['Close'])
    df['Thrusting'] = talib.CDLTHRUSTING(df['Open'], df['High'], df['Low'], df['Close'])
    df['Tristar'] = talib.CDLTRISTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['Unique_River'] = talib.CDLUNIQUE3RIVER(df['Open'], df['High'], df['Low'], df['Close'])
    df['Upside_Gap_Crows'] = talib.CDLUPSIDEGAP2CROWS(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Crows'] = talib.CDL3BLACKCROWS(df['Open'], df['High'], df['Low'], df['Close'])
    df['Up_Down_3Gap'] = talib.CDLXSIDEGAP3METHODS(df['Open'], df['High'], df['Low'], df['Close'])

    #
    # #Fractal Dimension
    # df['Hurst'] = df['Close'].rolling(window=100).apply(lambda x: compute_Hc(x, kind='price')[0], raw=False)
    # df['Fractal_Dimension'] = 2 - df['Hurst']

    #calculate interaction features
    df['Volume-ATR'] = df['Volume'] / df['ATR']
    df['VWAP-ATR'] = (df['Close'] - df['VWAP']) / df['ATR']
    df['RSI-MACD'] = df['RSI'] * df['MACD']
    df['Stochastic-RSI'] = df['Fast_%K'] / df['RSI']
    df['BB-KC'] = df['BB_Width'] / df['KC_Width']
    df['Ichimoku_Overlap'] = (df['Close'] - df['Kijun_Sen']) / (df['Senkou_Span_B'] - df['Senkou_Span_A'])
    df['LOW-HIGH_EMA'] = df['LOW_EMA'] / df['HIGH_EMA']

    #Candlestick interactions
    df['Closing_Marubozu-OBV'] = df['Closing_Marubozu'] * df['OBV']
    df['Closing_Marubozu-ATR'] = df['Closing_Marubozu'] * df['ATR']
    df['Closing_Marubozu-RSI'] = df['Closing_Marubozu'] * df['RSI']
    df['Marubozu-OBV'] = df['Marubozu'] * df['OBV']
    df['Marubozu-ATR'] = df['Marubozu'] * df['ATR']
    df['Marubozu-RSI'] = df['Marubozu'] * df['RSI']
    df['Short_Candle-OBV'] = df['Short_Candle'] * df['OBV']
    df['Short_Candle-ATR'] = df['Short_Candle'] * df['ATR']
    df['Short_Candle-RSI'] = df['Short_Candle'] * df['RSI']
    df['Long_Candle-OBV'] = df['Long_Candle'] * df['OBV']
    df['Long_Candle-ATR'] = df['Long_Candle'] * df['ATR']
    df['Long_Candle-RSI'] = df['Long_Candle'] * df['RSI']
    df['Doji-OBV'] = df['Doji'] * df['OBV']
    df['Doji-ATR'] = df['Doji'] * df['ATR']
    df['Doji-RSI'] = df['Doji'] * df['RSI']
    df['Long_Doji-OBV'] = df['Long_Doji'] * df['OBV']
    df['Long_Doji-ATR'] = df['Long_Doji'] * df['ATR']
    df['Long_Doji-RSI'] = df['Long_Doji'] * df['RSI']
    df['Dragonfly_Doji-OBV'] = df['Dragonfly_Doji'] * df['OBV']
    df['Dragonfly_Doji-ATR'] = df['Dragonfly_Doji'] * df['ATR']
    df['Dragonfly_Doji-RSI'] = df['Dragonfly_Doji'] * df['RSI']
    df['Belt_Hold-OBV'] = df['Belt_Hold'] * df['OBV']
    df['Belt_Hold-ATR'] = df['Belt_Hold'] * df['ATR']
    df['Belt_Hold-RSI'] = df['Belt_Hold'] * df['RSI']

    #Donchain Breakout
    df['Upper'] = df['Close'].rolling(288 - 1).max().shift(1)
    df['Lower'] = df['Close'].rolling(288 - 1).min().shift(1)
    df['Don_Signal'] = np.nan
    df.loc[df['Close'] > df['Upper'], 'Don_Signal'] = 1
    df.loc[df['Close'] < df['Lower'], 'Don_Signal'] = -1
    df['Don_Signal'] = df['Don_Signal'].ffill()
    df.drop(['Upper', 'Lower'], axis=1, inplace=True)

    # Generate Scree Plot
    # explained_variance_ratio = pca.explained_variance_ratio_
    # components = range(1, len(explained_variance_ratio) + 1)
    #
    # plt.figure(figsize=(8, 5))
    # plt.bar(components, explained_variance_ratio, color='blue', alpha=0.7, label='Explained Variance Ratio')
    # plt.plot(components, explained_variance_ratio, 'ro-', label='Cumulative Explained Variance')
    # plt.xlabel('Principal Components')
    # plt.ylabel('Explained Variance Ratio')
    # plt.title('Scree Plot for RSI PCA Analysis')
    # plt.legend()
    # plt.show()

    #drop temporary columns
    df.drop(['CumulativeVolume', 'Absolute_Momentum', 'Short_EMA_Momentum', 'Short_EMA_Absolute', 'Double_EMA_Momentum', 'Double_EMA_Absolute'], axis=1, inplace=True)
    df.replace([np.inf, -np.inf], 0, inplace=True)
    df.dropna(inplace=True)

    agg_excluded = ['Timestamp', 'Day', 'Session', 'Hour', 'Hour_Sin', 'Hour_Cos', 'Label', 'Price_Diff_Future_Abs', 'Price_Diff_Future']
    agg_period = [4, 16, 48, 96, 384, 1152] #1h, 4h, 12h, 1d, 4d, 12d
    agg_df = pd.DataFrame()
    for period in agg_period:
        for feature in df.columns:
            if feature not in agg_excluded:
                print(f"Calculating {feature} for period {period}...")
                agg_df[f'{feature}_Mean_{period}'] = df[feature].rolling(window=period).mean()
                agg_df[f'{feature}_Dev_{period}'] = df[feature].rolling(window=period).std()
                agg_df[f'{feature}_Drawdown_{period}'] = (df[feature] / df[feature].rolling(window=period).max() - 1).rolling(window=period).min()

    df['Label_Mean_4'] = df['Label'].shift(1).rolling(window=4).mean()
    df['Label_Mean_16'] = df['Label'].shift(1).rolling(window=16).mean()
    df['Label_Mean_48'] = df['Label'].shift(1).rolling(window=48).mean()
    df['Label_Mean_96'] = df['Label'].shift(1).rolling(window=96).mean()
    df['Label_Mean_384'] = df['Label'].shift(1).rolling(window=384).mean()
    df['Label_Mean_1152'] = df['Label'].shift(1).rolling(window=1152).mean()
    df['Label_Dev_4'] = df['Label'].shift(1).rolling(window=4).std()
    df['Label_Dev_16'] = df['Label'].shift(1).rolling(window=16).std()
    df['Label_Dev_48'] = df['Label'].shift(1).rolling(window=48).std()
    df['Label_Dev_96'] = df['Label'].shift(1).rolling(window=96).std()
    df['Label_Dev_384'] = df['Label'].shift(1).rolling(window=384).std()
    df['Label_Dev_1152'] = df['Label'].shift(1).rolling(window=1152).std()
    df['Label_Drawdown_4'] = (df['Label'].shift(1) / df['Label'].shift(1).rolling(window=4).max() - 1).rolling(window=4).min()
    df['Label_Drawdown_16'] = (df['Label'].shift(1) / df['Label'].shift(1).rolling(window=16).max() - 1).rolling(window=16).min()
    df['Label_Drawdown_48'] = (df['Label'].shift(1) / df['Label'].shift(1).rolling(window=48).max() - 1).rolling(window=48).min()
    df['Label_Drawdown_96'] = (df['Label'].shift(1) / df['Label'].shift(1).rolling(window=96).max() - 1).rolling(window=96).min()
    df['Label_Drawdown_384'] = (df['Label'].shift(1) / df['Label'].shift(1).rolling(window=384).max() - 1).rolling(window=384).min()
    df['Label_Drawdown_1152'] = (df['Label'].shift(1) / df['Label'].shift(1).rolling(window=1152).max() - 1).rolling(window=1152).min()

    df = pd.concat([df, agg_df], axis=1)
    df = df.dropna(axis=1, thresh=len(df) - 1452)
    df = df.fillna(0)

    #defragmenting dataframe
    df = df.copy()

    # PCA + k-means
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df)
    pca = PCA(n_components=6, random_state=42)
    pca_data = pca.fit_transform(df_scaled)
    pca_df = pd.DataFrame(pca_data, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6'])

    kmeans = KMeans(n_clusters=8, random_state=42)
    cluster_labels = kmeans.fit_predict(df_scaled)
    df['Cluster'] = cluster_labels

    df = pd.concat([df, pca_df], axis=1)
    df.dropna(inplace=True)

    training_df = df[df['Timestamp'] > 1672549200000] #January 1st 2023 12:15 am to present
    eval_df = df[((df['Timestamp'] > 1654056000000) & (df['Timestamp'] <= 1672549200000))] #June 1st 12:15am 2022 to Jan 1st 2023 12am
    test_df = df[((df['Timestamp'] >= 1641013200000) & (df['Timestamp'] <= 1654056000000))] #Jan 1st 12am 2022 to June 1st 12am 2022

    training_df.reset_index(drop=True, inplace=True)
    eval_df.reset_index(drop=True, inplace=True)
    test_df.reset_index(drop=True, inplace=True)

    # RSI PCA analysis
    rsis = pd.DataFrame()
    for p in range(2, 33):
        rsis[p] = talib.RSI(eval_df['Close'], timeperiod=p)

    rsi_means = rsis.mean()
    rsis -= rsi_means
    rsis = rsis.dropna()

    rsi_pca = PCA(n_components=3, random_state=42)
    pca_data = rsi_pca.fit_transform(rsis)
    rsi_pca_df = pd.DataFrame(pca_data, columns=['RSI_PC1', 'RSI_PC2', 'RSI_PC3'])
    eval_df = pd.concat([eval_df, rsi_pca_df], axis=1)

    training_rsis = pd.DataFrame()
    for p in range(2, 33):
        training_rsis[p] = talib.RSI(training_df['Close'], timeperiod=p)

    training_rsi_means = training_rsis.mean()
    training_rsis -= training_rsi_means
    training_rsis = training_rsis.dropna()

    pca_data = rsi_pca.transform(training_rsis)
    rsi_pca_df = pd.DataFrame(pca_data, columns=['RSI_PC1', 'RSI_PC2', 'RSI_PC3'])
    training_df = pd.concat([training_df, rsi_pca_df], axis=1)

    test_rsis = pd.DataFrame()
    for p in range(2, 33):
        test_rsis[p] = talib.RSI(test_df['Close'], timeperiod=p)

    test_rsi_means = test_rsis.mean()
    test_rsis -= test_rsi_means
    test_rsis = test_rsis.dropna()

    pca_data = rsi_pca.transform(test_rsis)
    rsi_pca_df = pd.DataFrame(pca_data, columns=['RSI_PC1', 'RSI_PC2', 'RSI_PC3'])
    test_df = pd.concat([test_df, rsi_pca_df], axis=1)

    # ATR PCA analysis
    atrs = pd.DataFrame()
    for p in range(2, 33):
        atrs[p] = talib.ATR(eval_df['High'], eval_df['Low'], eval_df['Close'], timeperiod=p)

    atr_means = atrs.mean()
    atrs -= atr_means
    atrs = atrs.dropna()

    atr_pca = PCA(n_components=3, random_state=42)
    pca_data = atr_pca.fit_transform(atrs)
    atr_pca_df = pd.DataFrame(pca_data, columns=['ATR_PC1', 'ATR_PC2', 'ATR_PC3'])
    eval_df = pd.concat([eval_df, atr_pca_df], axis=1)

    training_atrs = pd.DataFrame()
    for p in range(2, 33):
        training_atrs[p] = talib.ATR(training_df['High'], training_df['Low'], training_df['Close'], timeperiod=p)

    training_atr_means = training_atrs.mean()
    training_atrs -= training_atr_means
    training_atrs = training_atrs.dropna()

    pca_data = atr_pca.transform(training_atrs)
    atr_pca_df = pd.DataFrame(pca_data, columns=['ATR_PC1', 'ATR_PC2', 'ATR_PC3'])
    training_df = pd.concat([training_df, atr_pca_df], axis=1)

    test_atrs = pd.DataFrame()
    for p in range(2, 33):
        test_atrs[p] = talib.ATR(test_df['High'], test_df['Low'], test_df['Close'], timeperiod=p)

    test_atr_means = test_atrs.mean()
    test_atrs -= test_atr_means
    test_atrs = test_atrs.dropna()

    pca_data = atr_pca.transform(test_atrs)
    atr_pca_df = pd.DataFrame(pca_data, columns=['ATR_PC1', 'ATR_PC2', 'ATR_PC3'])
    test_df = pd.concat([test_df, atr_pca_df], axis=1)

    # ADX PCA analysis
    adxs = pd.DataFrame()
    for p in range(2, 33):
        adxs[p] = talib.ADX(eval_df['High'], eval_df['Low'], eval_df['Close'], timeperiod=p)

    adx_means = adxs.mean()
    adxs -= adx_means
    adxs = adxs.dropna()

    adx_pca = PCA(n_components=5, random_state=42)
    pca_data = adx_pca.fit_transform(adxs)
    adx_pca_df = pd.DataFrame(pca_data, columns=['ADX_PC1', 'ADX_PC2', 'ADX_PC3', 'ADX_PC4', 'ADX_PC5'])
    eval_df = pd.concat([eval_df, adx_pca_df], axis=1)

    training_adxs = pd.DataFrame()
    for p in range(2, 33):
        training_adxs[p] = talib.ADX(training_df['High'], training_df['Low'], training_df['Close'], timeperiod=p)

    training_adx_means = training_adxs.mean()
    training_adxs -= training_adx_means
    training_adxs = training_adxs.dropna()

    pca_data = adx_pca.transform(training_adxs)
    adx_pca_df = pd.DataFrame(pca_data, columns=['ADX_PC1', 'ADX_PC2', 'ADX_PC3', 'ADX_PC4', 'ADX_PC5'])
    training_df = pd.concat([training_df, adx_pca_df], axis=1)

    test_adxs = pd.DataFrame()
    for p in range(2, 33):
        test_adxs[p] = talib.ADX(test_df['High'], test_df['Low'], test_df['Close'], timeperiod=p)

    test_adx_means = test_adxs.mean()
    test_adxs -= test_adx_means
    test_adxs = test_adxs.dropna()

    pca_data = adx_pca.transform(test_adxs)
    adx_pca_df = pd.DataFrame(pca_data, columns=['ADX_PC1', 'ADX_PC2', 'ADX_PC3', 'ADX_PC4', 'ADX_PC5'])
    test_df = pd.concat([test_df, adx_pca_df], axis=1)

    training_df.dropna(inplace=True)
    eval_df.dropna(inplace=True)
    test_df.dropna(inplace=True)

    #calculate aggregate pca features
    agg_included = ['RSI_PC1', 'RSI_PC2', 'RSI_PC3', 'ATR_PC1', 'ATR_PC2', 'ATR_PC3', 'ADX_PC1', 'ADX_PC2', 'ADX_PC3', 'ADX_PC4', 'ADX_PC5']
    agg_period = [4, 16, 48, 96, 384, 1152]  # 1h, 4h, 12h, 1d, 4d, 12d
    training_agg_df = pd.DataFrame()
    eval_agg_df = pd.DataFrame()
    test_agg_df = pd.DataFrame()
    for period in agg_period:
        for feature in training_df.columns:
            if feature in agg_included:
                training_agg_df[f'{feature}_Mean_{period}'] = training_df[feature].rolling(window=period).mean()
                training_agg_df[f'{feature}_Dev_{period}'] = training_df[feature].rolling(window=period).std()
                training_agg_df[f'{feature}_Drawdown_{period}'] = (training_df[feature] / training_df[feature].rolling(window=period).max() - 1).rolling(window=period).min()
        for feature in eval_df.columns:
            if feature in agg_included:
                eval_agg_df[f'{feature}_Mean_{period}'] = eval_df[feature].rolling(window=period).mean()
                eval_agg_df[f'{feature}_Dev_{period}'] = eval_df[feature].rolling(window=period).std()
                eval_agg_df[f'{feature}_Drawdown_{period}'] = (eval_df[feature] / eval_df[feature].rolling(window=period).max() - 1).rolling(window=period).min()
        for feature in test_df.columns:
            if feature in agg_included:
                test_agg_df[f'{feature}_Mean_{period}'] = test_df[feature].rolling(window=period).mean()
                test_agg_df[f'{feature}_Dev_{period}'] = test_df[feature].rolling(window=period).std()
                test_agg_df[f'{feature}_Drawdown_{period}'] = (test_df[feature] / test_df[feature].rolling(window=period).max() - 1).rolling(window=period).min()

    training_df = pd.concat([training_df, training_agg_df], axis=1)
    training_df = training_df.dropna(axis=1, thresh=len(training_df) - 1452)
    training_df = training_df.fillna(0)
    eval_df = pd.concat([eval_df, eval_agg_df], axis=1)
    eval_df = eval_df.dropna(axis=1, thresh=len(eval_df) - 1452)
    eval_df = eval_df.fillna(0)
    test_df = pd.concat([test_df, test_agg_df], axis=1)
    test_df = test_df.dropna(axis=1, thresh=len(test_df) - 1452)
    test_df = test_df.fillna(0)

    ms = MinMaxScaler(feature_range=(0, 1))
    training_df['Weights'] = ms.fit_transform(training_df[['Price_Diff_Future_Abs']]) * 10
    eval_df['Weights'] = ms.fit_transform(eval_df[['Price_Diff_Future_Abs']]) * 10
    test_df['Weights'] = ms.fit_transform(test_df[['Price_Diff_Future_Abs']]) * 10

    training_df = training_df.copy()
    eval_df = eval_df.copy()
    test_df = test_df.copy()

    # count = 0
    # for column in df.columns:
    #     count += 1
    #     print(f"'{column}'", end=", ")
    #     if count == 15:
    #         count = 0
    #         print()
    return training_df, eval_df, test_df

def cmma(High, Low, Close, period, atr_period = 168):
    atr = talib.ATR(High, Low, Close, atr_period)
    ma = Close.rolling(period).mean()
    ind = (Close - ma) / (atr * period ** 0.5)
    return ind

def train_hmm(data, features, scaler, n_components=3):
    X = data[features].values
    X_scaled = scaler.fit_transform(X)
    model = hmm.GaussianHMM(n_components=n_components, covariance_type='full', n_iter=1000, random_state=42, verbose=True)
    model.fit(X_scaled)

    filepath = "hmm_model.pkl"
    with open("hmm_model.pkl", "wb") as f:
        pickle.dump(model, f)

    return filepath

def load_hmm(filepath):
    with open(filepath, "rb") as f:
        model = pickle.load(f)
    return model

def predict_states(model, data, features, scaler):
     X = data[features].values
     X_scaled = scaler.transform(X)
     states = model.predict_proba(X_scaled)
     return states

def plot_results_hmm(data, states, n_components):
    print("Plotting results...")
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,10), sharex=True)

    ax1.plot(data.index, data['Close'])
    ax1.set_title('Bitcoin Price and HMM States')
    ax1.set_ylabel('Price')

    for state in range(n_components):
        mask = (states == state)
        ax1.fill_between(data.index, data['Close'].min(), data['Close'].max(), where=mask, alpha=0.3, label=f'State {state}')
    ax1.legend()

    ax2.plot(data.index, data['Returns'])
    ax2.set_title('Bitcoin Returns')
    ax2.set_ylabel('Returns')
    ax2.set_xlabel('Date')

    plt.tight_layout()
    print("Saving plot...")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    plt.savefig('plots/hmm' + str(timestamp))

def plot_results_rf(data, labels_true, labels_pred):
    print("Plotting results...")
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,10), sharex=True)

    ax1.plot(data.index, data['Close'])
    ax1.set_title('Bitcoin Price and Real Labels')
    ax1.set_ylabel('Price')

    for label in range(3):
        mask = (labels_true == label)
        ax1.fill_between(data.index, data['Close'].min(), data['Close'].max(), where=mask, alpha=0.3, label=f'Signal {label}')
    ax1.legend()

    ax2.plot(data.index, data['Close'])
    ax2.set_title('Bitcoin Price and Predicted Labels')
    ax2.set_ylabel('Price')

    for label in range(3):
        mask = (labels_pred == label)
        ax2.fill_between(data.index, data['Close'].min(), data['Close'].max(), where=mask, alpha=0.3, label=f'Signal {label}')
    ax2.legend()

    plt.tight_layout()
    print("Saving plot...")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    plt.savefig('plots/rf' + str(timestamp))

def custom_pnl_objective(y_true, y_pred):
    y_true[y_true == 0] = -1
    p_buy = 1.0 / (1.0 + np.exp(-y_pred))
    prob_diff = 2.0 * p_buy - 1.0
    loss = -(prob_diff * y_true)

    grad = -((2 * np.exp(-y_pred) * y_true) / ((np.exp(-y_pred) + 1) ** 2))
    hess = -((2 * np.exp(-2 * y_pred) * y_true * (-np.exp(y_pred) + 1)) / (np.exp(-y_pred) + 1) ** 3)
    hess = np.maximum(hess, 1e-6)

    #print(f"y_true:{y_true}, y_pred:{y_pred}, p_buy:{p_buy}, prob_diff:{prob_diff}, grad:{grad}, hess:{hess}")
    #print(f"grad counts:{np.unique(grad, return_counts=True)}, true counts:{np.unique(y_true, return_counts=True)}, pred counts: {np.unique(y_pred, return_counts=True)}")

    return grad.astype(np.float32), hess.astype(np.float32)

# def train_xgboost(data_train, labels_train, data_test, labels_test, weights, splits, iters):
#     tscv = TimeSeriesSplit(n_splits=splits)
#     scale_weight = labels_train.count(1)/labels_train.count(0)
#     print(scale_weight)
#
#     pipeline = Pipeline(steps=[('xgb', XGBClassifier(random_state=42, eval_metric='auc', early_stopping_rounds=70, tree_method='hist', device='cuda', objective='binary:logistic'))])
#
#     search_space = {
#         'xgb__max_depth': Integer(3, 14),
#         'xgb__learning_rate': Real(0.0005, 1.0, prior='log-uniform'),
#         'xgb__subsample': Real(0.15, 1.0),
#         'xgb__colsample_bytree': Real(0.5, 1.0),
#         'xgb__colsample_bylevel': Real(0.5, 1.0),
#         'xgb__colsample_bynode': Real(0.5, 1.0),
#         'xgb__reg_alpha': Real(0.0, 8.0),
#         'xgb__reg_lambda': Real(0.0, 13.0),
#         'xgb__gamma': Real(0.0, 12.0),
#         'xgb__n_estimators': Integer(100, 5000),
#         'xgb__min_child_weight': Integer(1, 15),
#         'xgb__max_bin': Integer(128, 512),
#         'xgb__grow_policy': Categorical(["depthwise", "lossguide"]),
#         'xgb__max_delta_step': Integer(0, 10),
#         'xgb__scale_pos_weight': Real(1.03, 1.07),
#         'xgb__booster': Categorical(["gbtree", "dart"])
#     }
#     opt = BayesSearchCV(pipeline, search_space, cv=tscv, n_iter=iters, scoring='roc_auc', random_state=42)
#
#     opt.fit(data_train, labels_train, xgb__eval_set=[(data_test, labels_test)], xgb__sample_weight=weights)
#     print("Best estimator: ", opt.best_estimator_)
#     print("Best score: ", opt.best_score_)
#     print("Best params: ", opt.best_params_)
#     print(opt.score(data_test, labels_test))
#     labels_pred = opt.predict(data_test)
#     labels_pred = labels_pred.tolist()
#     print(labels_pred.count(0))
#     print(labels_pred.count(1))
#     print(opt.best_estimator_.steps)
#
#     xgboost_step = opt.best_estimator_.steps[0]
#     xgboost_model = xgboost_step[1]
#     plot_importance(xgboost_model, max_num_features=100)
#     plt.show()
#
#     # save the model
#     filepath = "xgboost_pipeline.pkl"
#     with open(filepath, "wb") as f:
#         pickle.dump(opt.best_estimator_, f)
#
#     return filepath

def train_xgboost(data_train, labels_train, data_test, labels_test, weights, splits, iters):
    params = {
        'max_depth': 14,
        'learning_rate': 0.029317166919399066,
        'subsample': 1.0,
        'colsample_bytree': 0.7587347450548942,
        'colsample_bylevel': 0.840082181391577,
        'colsample_bynode': 0.7635282333655302,
        'reg_alpha': 0.0,
        'reg_lambda': 13.0,
        'gamma': 0.0,
        'n_estimators': 5000,
        'min_child_weight': 1,
        'max_bin': 470,
        'grow_policy': "depthwise",
        'max_delta_step': 9,
        'scale_pos_weight': 1.05,
        'booster': 'gbtree'
    }

    # Create the XGBClassifier directly with these parameters (plus any fixed ones)
    model = XGBClassifier(
        random_state=42,
        eval_metric='auc',
        early_stopping_rounds=100,
        tree_method='hist',
        device='cuda',
        objective='binary:logistic',
        **params
    )

    # Fit the model (pass the eval_set and sample_weight as needed)
    model.fit(
        data_train,
        labels_train,
        eval_set=[(data_test, labels_test)],
        sample_weight=weights
    )

    plot_importance(model, max_num_features=100)
    plt.show()

    #save the model
    filepath = "xgboost_pipeline.pkl"
    with open(filepath, "wb") as f:
        pickle.dump(model, f)

    return filepath

def load_xgboost(filepath):
    with open(filepath, "rb") as f:
        model = pickle.load(f)
    return model

def get_binanceus_signature(data, secret):
    postdata = urllib.parse.urlencode(data)
    message = postdata.encode()
    byte_key = bytes(secret, 'UTF-8')
    mac = hmac.new(byte_key, message, hashlib.sha256).hexdigest()
    return mac

def exchange_btc(side, quoteOrderQty):
    data = {
        "symbol": 'BTCUSDC',
        "side": side,
        "type": 'MARKET',
        "quoteOrderQty": quoteOrderQty,
        "timestamp": int(round(time.time() * 1000))
    }
    headers = {
        'X-MBX-APIKEY': api_key
    }
    signature = get_binanceus_signature(data, api_sec)
    payload = {
        **data,
        "signature": signature,
    }
    req = requests.post(('https://api.binance.us/api/v3/order'), headers=headers, data=payload)
    return req.text

def get_historical_data(start_time, end_time, symbol='BTCUSDC'):
    url = 'https://api.binance.us/api/v3/klines'
    headers = {
        'X-MBX-APIKEY': api_key,
    }
    parameters = {
        'symbol': symbol,
        'interval': '15m',
        'startTime': str(start_time),
        'limit': '1000'
    }

    flag = True
    dataframes = []
    session = requests.Session()
    while(flag):
        session.headers.update(headers)

        try:
            response = session.get(url, params=parameters)
            data = json.loads(response.text)
        except (ConnectionError, requests.Timeout, requests.TooManyRedirects) as e:
            print(e)

        df = pd.DataFrame(columns=["Timestamp", "Open", "High", "Low", "Close", "Volume"])

        for i in range(len(data)):
            df.loc[len(df)] = {"Timestamp": int(data[i][0]),
                               "Open": float(data[i][1]),
                               "High": float(data[i][2]),
                               "Low": float(data[i][3]),
                               "Close": float(data[i][4]),
                               "Volume": float(data[i][5])}
            close_time = int(data[i][6])

            if close_time >= end_time:
                flag = False
        dataframes.append(df)

        start_time += (1000*15*60000) #add time in milliseconds
        parameters['startTime'] = start_time #update time

    combined_df = pd.concat(dataframes, ignore_index=True)
    return combined_df

def calculate_indicators(df, pca_training_df):
    eth_df = pd.read_csv('ETHUSDC15m_2020-2025.csv', names=['Timestamp', 'High', 'Low', 'Close'])
    eth_df.drop(0, inplace=True)
    eth_df = eth_df.astype(float)
    df['Label'] = (df['Close'].shift(-1) > df['Close']).astype(int)

    halving_dates = [
        datetime.datetime(2020, 5, 11),
        datetime.datetime(2024, 4, 20),
        datetime.datetime(2028, 2, 19)  # next estimated halving
    ]
    halving_cycle_duration = datetime.timedelta(days=365)

    quarters = []
    months = []
    days = []
    hours = []
    sessions = []
    in_halving_cycle = []

    for timestamp in df['Timestamp']:
        utc_time = datetime.datetime.utcfromtimestamp(int(timestamp) / 1000)
        month = utc_time.month
        quarter = (month - 1) // 3 + 1
        day = utc_time.weekday()
        hour = utc_time.hour

        if 0 <= hour < 9:
            session = 0
        elif 7 <= hour < 16:
            session = 1
        elif 12 <= hour < 21:
            session = 2
        else:
            session = 0

        in_cycle = False
        for halving_date in halving_dates:
            if halving_date <= utc_time <= halving_date + halving_cycle_duration:
                in_cycle = True
                break

        quarters.append(quarter)
        months.append(month)
        days.append(day)
        hours.append(hour)
        sessions.append(session)
        in_halving_cycle.append(in_cycle)

    df['Quarter'] = quarters
    df['Month'] = months
    df['Day'] = days
    df['Hour'] = hours
    df['Session'] = sessions
    df['In_Halving_Cycle'] = in_halving_cycle
    df['In_Halving_Cycle'] = df['In_Halving_Cycle'].astype(int)

    # Intramarket Difference
    period = 24
    btc_cmma = cmma(df['High'], df['Low'], df['Close'], period)
    eth_cmma = cmma(eth_df['High'], eth_df['Low'], eth_df['Close'], period)
    df['BTC-ETC_Diff'] = btc_cmma - eth_cmma

    df['Price_Diff'] = df['Close'].diff()
    df["Returns"] = df["Close"].pct_change()
    df["Volatility"] = df["Returns"].rolling(window=24).std()
    df['Price_Diff_Future'] = df['Price_Diff'].shift(-1)
    df['Price_Diff_Future_Abs'] = df['Price_Diff_Future'].abs()

    # prob(buy) - prob(sell) = prob(total)
    # if prob(total) is positive that means the buy signal is stronger, if negative that means the sell signal is stronger
    # prob(total) * price_diff

    # Replaces 0's with a number close to 0 to avoid infinity being present in Volume_Change
    # Since the label column has 0's present, we need to make sure that they are not replaced
    df['Volume'] = df['Volume'].replace(0, 0.00000000000000001)
    df["Volume_Change"] = df["Volume"].pct_change()

    # Need this because there was a weird error were the data in these columns were not classified as floats, this caused a problem with the pipeline as I'm not using a target encoder
    df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce')
    df['Returns'] = pd.to_numeric(df['Returns'], errors='coerce')

    df['OBV'] = talib.OBV(df['Close'], df['Volume'])

    df['MFV'] = (((df['Close'] - df['Low']) - (df['High'] - df['Close']) / (df['High'] - df['Low'])) * df['Volume'])
    df['MFV'] = df['MFV'].fillna(0)  # need this incase nans are generated by dividing by 0
    df['A/D'] = 0
    for i in range(2, len(df)):
        currentMFV = df.loc[i, 'MFV']
        df.loc[i, 'A/D'] = df.loc[i - 1, 'A/D'] + currentMFV
    df['CMF'] = df['MFV'].rolling(window=21).sum() / df['Volume'].rolling(window=21).sum()

    df['CumulativeVolume'] = 0
    for i in range(2, len(df)):
        new_volume = df.loc[i - 1, 'CumulativeVolume'] + df.loc[i, 'Volume']
        df.loc[i, 'CumulativeVolume'] = new_volume
        if i % 97 == 0:  # reset the cumulative volume everyday
            df.loc[i, 'CumulativeVolume'] = df.loc[i, 'Volume']

    df['VWAP'] = (((df['High'] + df['Low'] + df['Close']) / 3) * df['Volume']) / df['CumulativeVolume']

    df['LOW_EMA'] = talib.EMA(df['Close'], timeperiod=9)
    df['HIGH_EMA'] = talib.EMA(df['Close'], timeperiod=21)
    df['RSI'] = talib.RSI(df['Close'], timeperiod=14)
    df['Aroon'] = talib.AROONOSC(df['High'], df['Low'], timeperiod=14)
    df['Fast_%K'], df['Fast_%D'] = talib.STOCHF(df['High'], df['Low'], df['Close'], fastk_period=14)
    df['WILLR'] = talib.WILLR(df['High'], df['Low'], df['Close'])
    df['CCI'] = talib.CCI(df['High'], df['Low'], df['Close'])
    df['ATR'] = talib.ATR(df['High'], df['Low'], df['Close'])
    df['ADX'] = talib.ADX(df['High'], df['Low'], df['Close'])

    df['Momentum'] = df['Close'].diff()
    df['Absolute_Momentum'] = df['Momentum'].abs()
    df['Short_EMA_Momentum'] = talib.EMA(df['Momentum'], timeperiod=13)
    df['Short_EMA_Absolute'] = talib.EMA(df['Absolute_Momentum'], timeperiod=13)
    df['Double_EMA_Momentum'] = talib.EMA(df['Short_EMA_Momentum'], timeperiod=25)
    df['Double_EMA_Absolute'] = talib.EMA(df['Short_EMA_Absolute'], timeperiod=25)
    df['TSI'] = 100 * (df['Double_EMA_Momentum'] / df['Double_EMA_Absolute'])

    df['BB_Upper'], df['BB_Middle'], df['BB_Lower'] = talib.BBANDS(df['Close'], 20)
    df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']

    macd, macdsignal, macdhist = talib.MACDFIX(df['Close'])
    df['MACD'] = macd
    df['MACDSignal'] = macdsignal
    df['MACDHist'] = macdhist

    df['PP'] = (df['High'].shift(1) + df['Low'].shift(1) + df['Close'].shift(1)) / 3
    df['R1'] = 2 * df['PP'] - df['Low'].shift(1)
    df['R2'] = df['PP'] + (df['High'].shift(1) - df['Low'].shift(1))
    df['S1'] = 2 * df['PP'] - df['High'].shift(1)
    df['S2'] = df['PP'] - (df['High'].shift(1) - df['Low'].shift(1))

    # Ichimoku Cloud
    df['Tenkan_Sen'] = (df['High'].rolling(window=9).max() + df['Low'].rolling(window=9).min()) / 2
    df['Kijun_Sen'] = (df['High'].rolling(window=26).max() + df['Low'].rolling(window=26).min()) / 2
    df['Senkou_Span_A'] = (df['Tenkan_Sen'] + df['Kijun_Sen']) / 2
    df['Senkou_Span_B'] = (df['High'].rolling(window=52).max() + df['Low'].rolling(window=52).min()) / 2
    df['Chikou_Span'] = df['Close'].shift(-26)

    # Keltner Channels
    df['KC_Middle'] = talib.EMA(df['Close'], timeperiod=20)
    df['KC_Upper'] = df['KC_Middle'] + df['ATR'] * 2
    df['KC_Lower'] = df['KC_Middle'] - df['ATR'] * 2
    df['KC_Width'] = df['KC_Upper'] - df['KC_Lower']

    # max/min points
    close = df['Close'].values
    length = len(close)
    slope = (talib.EMA(close, timeperiod=50) - talib.EMA(close, timeperiod=10)).tolist()
    extrema_unconfirmed = [0] * length

    for i in range(1, length - 1):
        if slope[i] < 0 and slope[i - 1] >= 0:
            extrema_unconfirmed[i] = -1
        if slope[i] > 0 and slope[i - 1] <= 0:
            extrema_unconfirmed[i] = 1

    df['Extrema_Unconfirmed'] = extrema_unconfirmed

    df['Confirmed_Fractal_Peak'] = (
            (df['Close'] > df['Close'].shift(1)) &
            (df['Close'] > df['Close'].shift(2)) &
            (df['Close'] > df['Close'].shift(-1)) &
            (df['Close'] > df['Close'].shift(-2))
    )
    df['Confirmed_Fractal_Trough'] = (
            (df['Close'] < df['Close'].shift(1)) &
            (df['Close'] < df['Close'].shift(2)) &
            (df['Close'] < df['Close'].shift(-1)) &
            (df['Close'] < df['Close'].shift(-2))
    )
    df['Confirmed_Fractal_Peak'] = df['Confirmed_Fractal_Peak'].astype(int)
    df['Confirmed_Fractal_Trough'] = df['Confirmed_Fractal_Trough'].astype(int)
    df['Confirmed_Fractal_Peak'] = df['Confirmed_Fractal_Peak'].shift(2)
    df['Confirmed_Fractal_Trough'] = df['Confirmed_Fractal_Trough'].shift(2)

    # Candlestick patterns
    df['Body_Size'] = abs(df['Close'] - df['Open'])
    df['Upper_Wick'] = df['High'] - df[['Open', 'Close']].max(axis=1)
    df['Lower_Wick'] = df[['Open', 'Close']].min(axis=1) - df['Low']
    df['Range'] = df['High'] - df['Low']

    # Add small number to avoid division by zero
    df['Body_Ratio'] = df['Body_Size'] / (df['Range'] + 1e-9)
    df['Upper_Wick_Ratio'] = df['Upper_Wick'] / (df['Range'] + 1e-9)
    df['Lower_Wick_Ratio'] = df['Lower_Wick'] / (df['Range'] + 1e-9)

    df['Two_Crows'] = talib.CDL2CROWS(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Crows'] = talib.CDL3BLACKCROWS(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Inside'] = talib.CDL3INSIDE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Strike'] = talib.CDL3LINESTRIKE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Outside'] = talib.CDL3OUTSIDE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Stars_South'] = talib.CDL3STARSINSOUTH(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Soldiers'] = talib.CDL3WHITESOLDIERS(df['Open'], df['High'], df['Low'], df['Close'])
    df['Abandoned_Baby'] = talib.CDLABANDONEDBABY(df['Open'], df['High'], df['Low'], df['Close'])
    df['Advance_Block'] = talib.CDLADVANCEBLOCK(df['Open'], df['High'], df['Low'], df['Close'])
    df['Belt_Hold'] = talib.CDLBELTHOLD(df['Open'], df['High'], df['Low'], df['Close'])
    df['Breakaway'] = talib.CDLBREAKAWAY(df['Open'], df['High'], df['Low'], df['Close'])
    df['Closing_Marubozu'] = talib.CDLCLOSINGMARUBOZU(df['Open'], df['High'], df['Low'], df['Close'])
    df['Baby_Swallow'] = talib.CDLCONCEALBABYSWALL(df['Open'], df['High'], df['Low'], df['Close'])
    df['Counterattack'] = talib.CDLCOUNTERATTACK(df['Open'], df['High'], df['Low'], df['Close'])
    df['Dark_Cloud'] = talib.CDLDARKCLOUDCOVER(df['Open'], df['High'], df['Low'], df['Close'])
    df['Doji'] = talib.CDLDOJI(df['Open'], df['High'], df['Low'], df['Close'])
    df['Doji_Star'] = talib.CDLDOJISTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['Dragonfly_Doji'] = talib.CDLDRAGONFLYDOJI(df['Open'], df['High'], df['Low'], df['Close'])
    df['Engulfing'] = talib.CDLENGULFING(df['Open'], df['High'], df['Low'], df['Close'])
    df['Evening_Doji'] = talib.CDLEVENINGDOJISTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['Evening_Star'] = talib.CDLEVENINGSTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['Up_Down_Gap'] = talib.CDLGAPSIDESIDEWHITE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Gravestone_Doji'] = talib.CDLGRAVESTONEDOJI(df['Open'], df['High'], df['Low'], df['Close'])
    df['Hammer'] = talib.CDLHAMMER(df['Open'], df['High'], df['Low'], df['Close'])
    df['Hanging_Man'] = talib.CDLHANGINGMAN(df['Open'], df['High'], df['Low'], df['Close'])
    df['Harami'] = talib.CDLHARAMI(df['Open'], df['High'], df['Low'], df['Close'])
    df['Harami_Cross'] = talib.CDLHARAMICROSS(df['Open'], df['High'], df['Low'], df['Close'])
    df['High_Wave'] = talib.CDLHIGHWAVE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Hikkake'] = talib.CDLHIKKAKE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Modified_Hikkake'] = talib.CDLHIKKAKEMOD(df['Open'], df['High'], df['Low'], df['Close'])
    df['Homing_Pigeon'] = talib.CDLHOMINGPIGEON(df['Open'], df['High'], df['Low'], df['Close'])
    df['Identical_Crows'] = talib.CDLIDENTICAL3CROWS(df['Open'], df['High'], df['Low'], df['Close'])
    df['In_Neck'] = talib.CDLINNECK(df['Open'], df['High'], df['Low'], df['Close'])
    df['Inverted_Hammer'] = talib.CDLINVERTEDHAMMER(df['Open'], df['High'], df['Low'], df['Close'])
    df['Kicking'] = talib.CDLKICKING(df['Open'], df['High'], df['Low'], df['Close'])
    df['Kicking_Length'] = talib.CDLKICKINGBYLENGTH(df['Open'], df['High'], df['Low'], df['Close'])
    df['Ladder_Bottom'] = talib.CDLLADDERBOTTOM(df['Open'], df['High'], df['Low'], df['Close'])
    df['Long_Doji'] = talib.CDLLONGLEGGEDDOJI(df['Open'], df['High'], df['Low'], df['Close'])
    df['Long_Candle'] = talib.CDLLONGLINE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Marubozu'] = talib.CDLMARUBOZU(df['Open'], df['High'], df['Low'], df['Close'])
    df['Matching_Low'] = talib.CDLMATCHINGLOW(df['Open'], df['High'], df['Low'], df['Close'])
    df['Mat_Hold'] = talib.CDLMATHOLD(df['Open'], df['High'], df['Low'], df['Close'])
    df['Morning_Doji'] = talib.CDLMORNINGDOJISTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['Morning_Star'] = talib.CDLMORNINGSTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['On_Neck'] = talib.CDLONNECK(df['Open'], df['High'], df['Low'], df['Close'])
    df['Piercing'] = talib.CDLPIERCING(df['Open'], df['High'], df['Low'], df['Close'])
    df['Rickshaw_Man'] = talib.CDLRICKSHAWMAN(df['Open'], df['High'], df['Low'], df['Close'])
    df['Rising_Falling'] = talib.CDLRISEFALL3METHODS(df['Open'], df['High'], df['Low'], df['Close'])
    df['Separating_Lines'] = talib.CDLSEPARATINGLINES(df['Open'], df['High'], df['Low'], df['Close'])
    df['Shooting_Star'] = talib.CDLSHOOTINGSTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['Short_Candle'] = talib.CDLSHORTLINE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Spinning_Top'] = talib.CDLSPINNINGTOP(df['Open'], df['High'], df['Low'], df['Close'])
    df['Stalled'] = talib.CDLSTALLEDPATTERN(df['Open'], df['High'], df['Low'], df['Close'])
    df['Stick_Sandwich'] = talib.CDLSTICKSANDWICH(df['Open'], df['High'], df['Low'], df['Close'])
    df['Takuri'] = talib.CDLTAKURI(df['Open'], df['High'], df['Low'], df['Close'])
    df['Tasuki_Gap'] = talib.CDLTASUKIGAP(df['Open'], df['High'], df['Low'], df['Close'])
    df['Thrusting'] = talib.CDLTHRUSTING(df['Open'], df['High'], df['Low'], df['Close'])
    df['Tristar'] = talib.CDLTRISTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['Unique_River'] = talib.CDLUNIQUE3RIVER(df['Open'], df['High'], df['Low'], df['Close'])
    df['Upside_Gap_Crows'] = talib.CDLUPSIDEGAP2CROWS(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Crows'] = talib.CDL3BLACKCROWS(df['Open'], df['High'], df['Low'], df['Close'])
    df['Up_Down_3Gap'] = talib.CDLXSIDEGAP3METHODS(df['Open'], df['High'], df['Low'], df['Close'])

    #
    # #Fractal Dimension
    # df['Hurst'] = df['Close'].rolling(window=100).apply(lambda x: compute_Hc(x, kind='price')[0], raw=False)
    # df['Fractal_Dimension'] = 2 - df['Hurst']

    # calculate interaction features
    df['Volume-ATR'] = df['Volume'] / df['ATR']
    df['VWAP-ATR'] = (df['Close'] - df['VWAP']) / df['ATR']
    df['RSI-MACD'] = df['RSI'] * df['MACD']
    df['Stochastic-RSI'] = df['Fast_%K'] / df['RSI']
    df['BB-KC'] = df['BB_Width'] / df['KC_Width']
    df['Ichimoku_Overlap'] = (df['Close'] - df['Kijun_Sen']) / (df['Senkou_Span_B'] - df['Senkou_Span_A'])
    df['LOW-HIGH_EMA'] = df['LOW_EMA'] / df['HIGH_EMA']

    # Candlestick interactions
    df['Closing_Marubozu-OBV'] = df['Closing_Marubozu'] * df['OBV']
    df['Closing_Marubozu-ATR'] = df['Closing_Marubozu'] * df['ATR']
    df['Closing_Marubozu-RSI'] = df['Closing_Marubozu'] * df['RSI']
    df['Marubozu-OBV'] = df['Marubozu'] * df['OBV']
    df['Marubozu-ATR'] = df['Marubozu'] * df['ATR']
    df['Marubozu-RSI'] = df['Marubozu'] * df['RSI']
    df['Short_Candle-OBV'] = df['Short_Candle'] * df['OBV']
    df['Short_Candle-ATR'] = df['Short_Candle'] * df['ATR']
    df['Short_Candle-RSI'] = df['Short_Candle'] * df['RSI']
    df['Long_Candle-OBV'] = df['Long_Candle'] * df['OBV']
    df['Long_Candle-ATR'] = df['Long_Candle'] * df['ATR']
    df['Long_Candle-RSI'] = df['Long_Candle'] * df['RSI']
    df['Doji-OBV'] = df['Doji'] * df['OBV']
    df['Doji-ATR'] = df['Doji'] * df['ATR']
    df['Doji-RSI'] = df['Doji'] * df['RSI']
    df['Long_Doji-OBV'] = df['Long_Doji'] * df['OBV']
    df['Long_Doji-ATR'] = df['Long_Doji'] * df['ATR']
    df['Long_Doji-RSI'] = df['Long_Doji'] * df['RSI']
    df['Dragonfly_Doji-OBV'] = df['Dragonfly_Doji'] * df['OBV']
    df['Dragonfly_Doji-ATR'] = df['Dragonfly_Doji'] * df['ATR']
    df['Dragonfly_Doji-RSI'] = df['Dragonfly_Doji'] * df['RSI']
    df['Belt_Hold-OBV'] = df['Belt_Hold'] * df['OBV']
    df['Belt_Hold-ATR'] = df['Belt_Hold'] * df['ATR']
    df['Belt_Hold-RSI'] = df['Belt_Hold'] * df['RSI']

    # Donchain Breakout
    df['Upper'] = df['Close'].rolling(288 - 1).max().shift(1)
    df['Lower'] = df['Close'].rolling(288 - 1).min().shift(1)
    df['Don_Signal'] = np.nan
    df.loc[df['Close'] > df['Upper'], 'Don_Signal'] = 1
    df.loc[df['Close'] < df['Lower'], 'Don_Signal'] = -1
    df['Don_Signal'] = df['Don_Signal'].ffill()
    df.drop(['Upper', 'Lower'], axis=1, inplace=True)

    # Generate Scree Plot
    # explained_variance_ratio = pca.explained_variance_ratio_
    # components = range(1, len(explained_variance_ratio) + 1)
    #
    # plt.figure(figsize=(8, 5))
    # plt.bar(components, explained_variance_ratio, color='blue', alpha=0.7, label='Explained Variance Ratio')
    # plt.plot(components, explained_variance_ratio, 'ro-', label='Cumulative Explained Variance')
    # plt.xlabel('Principal Components')
    # plt.ylabel('Explained Variance Ratio')
    # plt.title('Scree Plot for RSI PCA Analysis')
    # plt.legend()
    # plt.show()

    # drop temporary columns
    df.drop(['CumulativeVolume', 'Absolute_Momentum', 'Short_EMA_Momentum', 'Short_EMA_Absolute', 'Double_EMA_Momentum',
             'Double_EMA_Absolute'], axis=1, inplace=True)
    df.replace([np.inf, -np.inf], 0, inplace=True)
    df.dropna(inplace=True)

    # PCA + k-means
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df)
    pca = PCA(n_components=6)
    pca_data = pca.fit_transform(df_scaled)
    pca_df = pd.DataFrame(pca_data, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6'])

    kmeans = KMeans(n_clusters=8, random_state=42)
    cluster_labels = kmeans.fit_predict(df_scaled)
    df['Cluster'] = cluster_labels

    df = pd.concat([df, pca_df], axis=1)
    df.dropna(inplace=True)

    # RSI PCA analysis
    rsis = pd.DataFrame()
    training_rsis = pd.DataFrame()
    for p in range(2, 33):
        rsis[p] = talib.RSI(df['Close'], timeperiod=p)
        training_rsis[p] = talib.RSI(pca_training_df['Close'], timeperiod=p)

    rsi_means = rsis.mean()
    rsis -= rsi_means
    rsis = rsis.dropna()
    training_rsi_means = training_rsis.mean()
    training_rsis -= training_rsi_means
    training_rsis = training_rsis.dropna()

    rsi_pca = PCA(n_components=3, random_state=42)
    rsi_pca.fit(training_rsis)
    pca_data = rsi_pca.transform(rsis)
    rsi_pca_df = pd.DataFrame(pca_data, columns=['RSI_PC1', 'RSI_PC2', 'RSI_PC3'])
    df = pd.concat([df, rsi_pca_df], axis=1)

    # ATR PCA analysis
    atrs = pd.DataFrame()
    training_atrs = pd.DataFrame()
    for p in range(2, 33):
        atrs[p] = talib.ATR(df['High'], df['Low'], df['Close'], timeperiod=p)
        training_atrs[p] = talib.ATR(pca_training_df['High'], pca_training_df['Low'], pca_training_df['Close'], timeperiod=p)

    atr_means = atrs.mean()
    atrs -= atr_means
    atrs = atrs.dropna()
    training_atr_means = training_atrs.mean()
    training_atrs -= training_atr_means
    training_atrs = training_atrs.dropna()

    atr_pca = PCA(n_components=3, random_state=42)
    atr_pca.fit(training_atrs)
    pca_data = atr_pca.transform(atrs)
    atr_pca_df = pd.DataFrame(pca_data, columns=['ATR_PC1', 'ATR_PC2', 'ATR_PC3'])
    df = pd.concat([df, atr_pca_df], axis=1)

    # ADX PCA analysis
    adxs = pd.DataFrame()
    training_adxs = pd.DataFrame()
    for p in range(2, 33):
        adxs[p] = talib.ADX(df['High'], df['Low'], df['Close'], timeperiod=p)
        training_adxs[p] = talib.ADX(pca_training_df['High'], pca_training_df['Low'], pca_training_df['Close'], timeperiod=p)

    adx_means = adxs.mean()
    adxs -= adx_means
    adxs = adxs.dropna()
    training_adx_means = training_adxs.mean()
    training_adxs -= training_adx_means
    training_adxs = training_adxs.dropna()

    adx_pca = PCA(n_components=5, random_state=42)
    adx_pca.fit(training_adxs)
    pca_data = adx_pca.transform(adxs)
    adx_pca_df = pd.DataFrame(pca_data, columns=['ADX_PC1', 'ADX_PC2', 'ADX_PC3', 'ADX_PC4', 'ADX_PC5'])
    df = pd.concat([df, adx_pca_df], axis=1)

    agg_excluded = ['Timestamp', 'Day', 'Session', 'Hour', 'Hour_Sin', 'Hour_Cos', 'Label', 'Price_Diff_Future_Abs',
                    'Price_Diff_Future', 'Cluster', 'PC1', 'PC2', 'PC3', 'PC5', 'PC6']
    agg_period = [4, 16, 48, 96, 384, 1152]  # 1h, 4h, 12h, 1d, 4d, 12d
    agg_df = pd.DataFrame()
    for period in agg_period:
        for feature in df.columns:
            if feature not in agg_excluded:
                print(f"Calculating {feature} for period {period}...")
                agg_df[f'{feature}_Mean_{period}'] = df[feature].rolling(window=period).mean()
                agg_df[f'{feature}_Dev_{period}'] = df[feature].rolling(window=period).std()
                agg_df[f'{feature}_Drawdown_{period}'] = (df[feature] / df[feature].rolling(window=period).max() - 1).rolling(window=period).min()

    df['Label_Mean_4'] = df['Label'].shift(1).rolling(window=4).mean()
    df['Label_Mean_16'] = df['Label'].shift(1).rolling(window=16).mean()
    df['Label_Mean_48'] = df['Label'].shift(1).rolling(window=48).mean()
    df['Label_Mean_96'] = df['Label'].shift(1).rolling(window=96).mean()
    df['Label_Mean_384'] = df['Label'].shift(1).rolling(window=384).mean()
    df['Label_Mean_1152'] = df['Label'].shift(1).rolling(window=1152).mean()
    df['Label_Dev_4'] = df['Label'].shift(1).rolling(window=4).std()
    df['Label_Dev_16'] = df['Label'].shift(1).rolling(window=16).std()
    df['Label_Dev_48'] = df['Label'].shift(1).rolling(window=48).std()
    df['Label_Dev_96'] = df['Label'].shift(1).rolling(window=96).std()
    df['Label_Dev_384'] = df['Label'].shift(1).rolling(window=384).std()
    df['Label_Dev_1152'] = df['Label'].shift(1).rolling(window=1152).std()
    df['Label_Drawdown_4'] = (df['Label'].shift(1) / df['Label'].shift(1).rolling(window=4).max() - 1).rolling(
        window=4).min()
    df['Label_Drawdown_16'] = (df['Label'].shift(1) / df['Label'].shift(1).rolling(window=16).max() - 1).rolling(
        window=16).min()
    df['Label_Drawdown_48'] = (df['Label'].shift(1) / df['Label'].shift(1).rolling(window=48).max() - 1).rolling(
        window=48).min()
    df['Label_Drawdown_96'] = (df['Label'].shift(1) / df['Label'].shift(1).rolling(window=96).max() - 1).rolling(
        window=96).min()
    df['Label_Drawdown_384'] = (df['Label'].shift(1) / df['Label'].shift(1).rolling(window=384).max() - 1).rolling(
        window=384).min()
    df['Label_Drawdown_1152'] = (df['Label'].shift(1) / df['Label'].shift(1).rolling(window=1152).max() - 1).rolling(
        window=1152).min()

    df = pd.concat([df, agg_df], axis=1)
    df = df.dropna(axis=1, thresh=len(df) - 1452)
    df = df.fillna(0)

    # defragmenting dataframe
    df = df.copy()
    print(f"Number of rows with NaN values: {df.isna().any(axis=1).sum()}")

    return df

def convert_data_to_windows(data, window_size=2):
    rows = []
    for i in range(len(data) - window_size - 1):
        row = {}

        for feature in data.columns:
            if feature != 'Label' and feature != 'Weights':
                for t in range(window_size):
                    row[f"{feature}_t-{window_size - t}"] = data[feature].iloc[i + t + 1]

        row['Label'] = int(data['Label'].iloc[i + window_size])
        rows.append(row)

    window_data = pd.DataFrame(rows)
    window_data['Weights'] = data['Weights']

    return window_data

def trading_simulation(probas, closes, buy_percentage, sell_percentage, overconfidence_mult, starting_money=500, spend_percentage=0.1, holdout_threshold=0.02):
    money = starting_money
    bitcoin = 0
    probas = probas
    close_prices = closes
    buy_order = False
    sell_order = False

    #0 is sell, 2 is buy
    count = 0
    day_count = 0
    overconfidence = 0
    last_bought_price = 0
    last_sold_price = 0
    profitable_trade_count = 0
    trade_count = 0
    historical_daily_starting_assets = []
    for proba in probas:
        if count % 96 == 0: #update daily starting assets at the start of each day
            daily_starting_assets = money + (bitcoin * close_prices[count])
            historical_daily_starting_assets.append(daily_starting_assets)
            # if len(historical_daily_starting_assets) >= 2:
            #     if (historical_daily_starting_assets[day_count - 1] * 0.95) > historical_daily_starting_assets[day_count]:
            #         print(f"Stop-loss triggered on Day {day_count}: more than 5% drop in a single day.")
            #         print(f"Current money: {money}, Current bitcoin: {bitcoin}, Current count: {count}")
            #         return
            day_count += 1

        # hold = False
        # if (probability[0] >= (0.5 - holdout_threshold) and (probability[0] <= (0.5 - holdout_threshold))) and (probability[1] >= (0.5 - holdout_threshold) and (probability[1] <= (0.5 - holdout_threshold))):
        #     hold = True

        if proba[0] >= sell_percentage and sell_order == False:
            sell_order = True
        elif proba[1] >= buy_percentage and buy_order == False:
            overconfidence = (proba[1] - buy_percentage) * overconfidence_mult
            #print(f"Overconfidence on Day {count}: {overconfidence}")
            buy_order = True

        #print(hold)
        #print(buy_sell)
        if sell_order and proba[1] >= buy_percentage:
            money += bitcoin * close_prices[count]
            last_sold_price = close_prices[count]
            print(f"Day {day_count}: SOLD {bitcoin} BTC at {last_sold_price} each.")
            bitcoin = 0
            sell_order = False
        elif buy_order and proba[0] >= sell_percentage:
            amount_to_spend = money * (spend_percentage + overconfidence)
            # Avoid spending more than we have if overconfidence is large
            amount_to_spend = min(amount_to_spend, money)

            bitcoin_bought = amount_to_spend / close_prices[count]
            bitcoin += bitcoin_bought
            money -= amount_to_spend
            buy_order = False
            last_bought_price = close_prices[count]
            print(f"Day {day_count}: BOUGHT {bitcoin_bought} BTC at {last_bought_price} each.")

        if last_sold_price > last_bought_price:
            profitable_trade_count += 1
            trade_count += 1
        else:
            trade_count += 1

        print(f"Money: {money}, Bitcoin: {bitcoin}, Count: {count}")
        count += 1

    total_assets = money + bitcoin * close_prices[count-2]
    print("Final money: ", total_assets)
    print("Profit: ", total_assets - starting_money)
    print("Percentage profitable: ", profitable_trade_count / trade_count)

def get_next_interval(interval_seconds):
    now = time.time()
    next_interval = ((now // interval_seconds) + 1) * interval_seconds
    return next_interval - now

def notify():
    root = tk.Tk()
    root.withdraw()
    messagebox.showinfo("Notification", "Your code has finished running!")
    root.destroy()

window_size = 33
data_train, data_eval, data_test = load_and_preprocess_data("BTCUSDC15m_2020-2025_labeled.csv")
scaler = StandardScaler()
hmm_features = ['Close', 'Session', 'BTC-ETC_Diff', 'Price_Diff', 'Returns', 'Volatility', 'Don_Signal', 'RSI', 'Upper_Wick_Ratio', 'Volume_Change', 'Body_Ratio', 'Lower_Wick_Ratio', 'Upper_Wick', 'Body_Size', 'VWAP', 'Lower_Wick', 'Fast_%K',
                'Volume-ATR', 'CCI', 'Volume', 'BB-KC', 'Range', 'MACDHist', 'MACDSignal', 'MACD', 'Stochastic-RSI', 'RSI_PC1', 'RSI_PC2', 'RSI_PC3', 'ATR_PC1', 'ATR_PC2', 'ATR_PC3', 'ADX_PC1', 'ADX_PC2', 'ADX_PC3', 'ADX_PC4', 'ADX_PC5',
                'Cluster', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'RSI_Mean_4', 'Chikou_Span_Dev_48', 'Chikou_Span_Drawdown_4', 'KC_Upper_Drawdown_4']
xg_features = ['Close', 'Session', 'BTC-ETC_Diff', 'Price_Diff', 'Returns', 'Volatility', 'Closing_Marubozu', 'Doji', 'Dragonfly_Doji', 'Engulfing', 'Hikkake', 'Long_Doji', 'Long_Candle', 'Short_Candle', 'Marubozu', 'Harami', 'Don_Signal',
               'RSI', 'Upper_Wick_Ratio', 'Volume_Change', 'Body_Ratio', 'Lower_Wick_Ratio', 'Upper_Wick', 'Body_Size', 'VWAP', 'Lower_Wick', 'Fast_%K', 'Volume-ATR', 'CCI', 'Volume', 'BB-KC', 'Range', 'MACDHist', 'MACDSignal', 'MACD',
               'Stochastic-RSI', 'RSI_PC1', 'RSI_PC2', 'RSI_PC3', 'ATR_PC1', 'ATR_PC2', 'ATR_PC3', 'ADX_PC1', 'ADX_PC2', 'ADX_PC3', 'ADX_PC4', 'ADX_PC5', 'Cluster', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'Belt_Hold',
               'Weights', 'Label']
agg_features = ['RSI_Mean_4', 'RSI_Mean_16', 'TSI_Mean_16', 'Volume_Change_Mean_4', 'Price_Diff_Mean_4', 'TSI_Mean_4', 'Returns_Mean_4', 'Lower_Wick_Ratio_Mean_4',
                'BB-KC_Mean_96', 'Returns_Mean_16', 'Price_Diff_Mean_16', 'Body_Ratio_Mean_4', 'Label_Mean_16', 'Price_Diff_Mean_96', 'CCI_Mean_4', 'MACDHist_Mean_96', 'CCI_Mean_96', 'Doji-RSI_Mean_4', 'Belt_Hold-ATR_Mean_48', 'Label_Mean_96',
                'VWAP_Mean_48', 'Long_Doji-RSI_Mean_4', 'Short_Candle-RSI_Mean_4', 'Aroon_Mean_4', 'CCI_Mean_16', 'Long_Candle-RSI_Mean_4', 'ADX_Mean_4', 'Aroon_Mean_16', 'Volume_Change_Mean_16', 'MACDSignal_Mean_4',
                'Stochastic-RSI_Mean_48', 'MACDSignal_Mean_16', 'Closing_Marubozu-ATR_Mean_4', 'Volume-ATR_Mean_4', 'Upper_Wick_Ratio_Mean_4', 'Upper_Wick_Mean_4', 'Returns_Mean_48', 'Marubozu-OBV_Mean_16',
                'Dragonfly_Doji-RSI_Mean_48', 'Ichimoku_Overlap_Mean_48', 'Lower_Wick_Ratio_Mean_48', 'RSI_Mean_48', 'Price_Diff_Mean_48', 'Fast_%K_Mean_4', 'Aroon_Mean_96', 'Hammer_Mean_16', 'Body_Ratio_Mean_16',
                'Engulfing_Mean_4', 'VWAP_Mean_4', 'Belt_Hold-ATR_Mean_96', 'Returns_Mean_96', 'Marubozu-RSI_Mean_48', 'Hikkake_Mean_16', 'LOW-HIGH_EMA_Mean_4', 'Chikou_Span_Dev_48', 'Long_Doji-RSI_Dev_4',
                'Doji-RSI_Dev_48', 'Long_Candle-RSI_Dev_48', 'Stochastic-RSI_Dev_96', 'Closing_Marubozu-RSI_Dev_48', 'ADX_Dev_4', 'Stochastic-RSI_Dev_4', 'Lower_Wick_Ratio_Dev_4', 'Long_Candle_Dev_384',
                'ADX_Dev_96', 'High_Dev_4', 'TSI_Dev_48', 'Engulfing_Dev_48', 'VWAP_Dev_48', 'Volume-ATR_Dev_4', 'OBV_Dev_4', 'BB_Upper_Dev_4', 'TSI_Dev_4', 'BB_Lower_Dev_4', 'Fast_%D_Dev_4', 'KC_Lower_Dev_4',
                'Volume_Dev_4', 'Long_Doji-RSI_Dev_16', 'Senkou_Span_A_Dev_4', 'Senkou_Span_B_Dev_16', 'CCI_Dev_4', 'Aroon_Dev_96', 'Stochastic-RSI_Dev_48', 'Lower_Wick_Ratio_Dev_48', 'BB-KC_Dev_16', 'Upper_Wick_Dev_4',
                'Dragonfly_Doji-OBV_Dev_48', 'Belt_Hold-RSI_Dev_48', 'Kijun_Sen_Dev_16', 'Closing_Marubozu-RSI_Dev_4', 'Upper_Wick_Ratio_Dev_4', 'Short_Candle-RSI_Dev_96', 'Long_Doji-RSI_Dev_48', 'Long_Candle-RSI_Dev_16', 'Closing_Marubozu-RSI_Dev_16',
                'Fast_%K_Dev_48', 'MACDSignal_Dev_4', 'CCI_Dev_48', 'Belt_Hold-RSI_Dev_4', 'RSI_Dev_48', 'Short_Candle-OBV_Dev_16', 'VWAP-ATR_Dev_4', 'Long_Candle-RSI_Dev_384', 'Closing_Marubozu-RSI_Dev_96', 'BTC-ETC_Diff_Dev_4', 'Doji-RSI_Dev_4',
                'Chikou_Span_Drawdown_4', 'KC_Upper_Drawdown_4', 'Volatility_Drawdown_4', 'CMF_Drawdown_4', 'ADX_Drawdown_4', 'ATR_Drawdown_4', 'S2_Drawdown_4', 'KC_Upper_Drawdown_16', 'Open_Drawdown_4', 'Fast_%D_Drawdown_4', 'ADX_Drawdown_16', 'BB_Lower_Drawdown_4',
                'RSI-MACD_Drawdown_4', 'Chikou_Span_Drawdown_16', 'BB_Upper_Drawdown_4', 'Volatility_Drawdown_16', 'R2_Drawdown_4', 'RSI_Drawdown_4', 'BB_Upper_Drawdown_16', 'BB-KC_Drawdown_4', 'BB_Lower_Drawdown_16', 'CMF_Drawdown_16', 'S1_Drawdown_4',
                'Low_Drawdown_4', 'VWAP-ATR_Drawdown_16', 'BB_Middle_Drawdown_4', 'R2_Drawdown_16', 'Senkou_Span_B_Drawdown_4', 'High_Drawdown_4', 'Chikou_Span_Drawdown_48', 'RSI_PC3_Mean_16', 'RSI_PC1_Mean_48', 'RSI_PC1_Mean_16', 'RSI_PC2_Mean_16', 'ATR_PC2_Mean_4',
                'ADX_PC2_Mean_384', 'ATR_PC3_Mean_16', 'ADX_PC1_Mean_96', 'ADX_PC5_Mean_16', 'RSI_PC1_Mean_384', 'ADX_PC3_Mean_4', 'ADX_PC2_Mean_16', 'RSI_PC3_Mean_4', 'ATR_PC3_Mean_96', 'RSI_PC2_Dev_48', 'RSI_PC1_Dev_48', 'RSI_PC3_Dev_48', 'ADX_PC2_Mean_4', 'RSI_PC1_Dev_96',
                'ATR_PC3_Dev_4', 'ADX_PC4_Dev_48', 'ADX_PC1_Dev_4', 'RSI_PC1_Dev_16', 'RSI_PC2_Dev_96', 'ADX_PC4_Dev_4', 'ADX_PC2_Dev_16', 'ADX_PC3_Dev_96', 'ADX_PC5_Dev_48', 'ADX_PC5_Dev_16', 'ATR_PC2_Drawdown_4', 'ADX_PC4_Drawdown_4', 'RSI_PC3_Drawdown_4', 'RSI_PC3_Drawdown_16',
                'ATR_PC3_Drawdown_16', 'ATR_PC1_Drawdown_4', 'ADX_PC5_Drawdown_4']

hmm_comps = 2
hmm_path = train_hmm(pd.concat([data_test, data_eval, data_train], ignore_index=True), hmm_features, scaler, hmm_comps)
hmm_model = load_hmm(hmm_path)

#walk forward hmm prediction
# states = []
# data_train_current = pd.DataFrame(columns=data_train[hmm_features].columns)
# train_count = 0
# for index, row in data_train[hmm_features].iterrows():
#     train_count += 1
#     print(train_count)
#     data_train_current = pd.concat([data_train_current, pd.DataFrame([row.to_dict()])], ignore_index=True)
#     states.append(predict_states(hmm_model, data_train_current, hmm_features, scaler)[-1])
#     #keeps a window of 500 to improve computational efficiency
#     if train_count >= 1000:
#         data_train_current = data_train_current.drop(data_train_current.index[0])
#
# states_eval = []
# data_eval_current = pd.DataFrame(columns=data_eval[hmm_features].columns)
# eval_count = 0
# for index, row in data_eval[hmm_features].iterrows():
#     eval_count += 1
#     print(eval_count)
#     data_eval_current = pd.concat([data_eval_current, pd.DataFrame([row.to_dict()])], ignore_index=True)
#     states_eval.append(predict_states(hmm_model, data_eval_current, hmm_features, scaler)[-1])
#     if eval_count >= 1000:
#         data_eval_current = data_eval_current.drop(data_eval_current.index[0])
#
# states_test = []
# data_test_current = pd.DataFrame(columns=data_test[hmm_features].columns)
# test_count = 0
# for index, row in data_test[hmm_features].iterrows():
#     test_count += 1
#     print(test_count)
#     data_test_current = pd.concat([data_test_current, pd.DataFrame([row.to_dict()])], ignore_index=True)
#     states_test.append(predict_states(hmm_model, data_test_current, hmm_features, scaler)[-1])
#     if test_count >= 1000:
#          data_test_current = data_test_current.drop(data_test_current.index[0])
#
# data_train_states = list(zip(*states))
# data_eval_states = list(zip(*states_eval))
# data_test_states = list(zip(*states_test))
#
# for i in range(hmm_comps):
#     data_train[f'State{i}'] = data_train_states[i]
#     data_eval[f'State{i}'] = data_eval_states[i]
#     data_test[f'State{i}'] = data_test_states[i]
#     xg_features.append(f'State{i}')
#
# #defragmenting the dataframes
# data_train = data_train.copy()
# data_eval = data_eval.copy()
# data_test = data_test.copy()
#
# #Build xgboost model
# xg_data_train = convert_data_to_windows(data_train[xg_features], window_size)
# xg_data_eval = convert_data_to_windows(data_eval[xg_features], window_size)
# xg_data_test = convert_data_to_windows(data_test[xg_features], window_size)
#
# #dropping the first few rows to line up data with the xgboost data
# data_train = data_train.drop(index=data_train.index[:window_size])
# data_eval = data_eval.drop(index=data_eval.index[:window_size])
# data_test = data_test.drop(index=data_test.index[:window_size])
#
# #need to reset index so that the concat works properly
# xg_data_train = xg_data_train.reset_index(drop=True)
# data_train = data_train.reset_index(drop=True)
# xg_data_eval = xg_data_eval.reset_index(drop=True)
# data_eval = data_eval.reset_index(drop=True)
# xg_data_test = xg_data_test.reset_index(drop=True)
# data_test = data_test.reset_index(drop=True)
# xg_data_train = pd.concat([xg_data_train, data_train[agg_features]], axis=1)
# xg_data_eval = pd.concat([xg_data_eval, data_eval[agg_features]], axis=1)
# xg_data_test = pd.concat([xg_data_test, data_test[agg_features]], axis=1)
#
# xg_data_train.dropna(inplace=True)
# xg_data_eval.dropna(inplace=True)
# xg_data_test.dropna(inplace=True)
#
# xg_labels_train = xg_data_train.pop('Label').tolist()
# xg_labels_train = [int(x) for x in xg_labels_train]
# xg_labels_eval = xg_data_eval.pop('Label').tolist()
# xg_labels_eval = [int(x) for x in xg_labels_eval]
# xg_labels_test = xg_data_test.pop('Label').tolist()
# xg_labels_test = [int(x) for x in xg_labels_test]
#
# weights = xg_data_train['Weights'].tolist()
# xg_data_train.drop(columns='Weights', inplace=True)
# xg_data_eval.drop(columns='Weights', inplace=True)
# xg_data_test.drop(columns='Weights', inplace=True)
# xgboost_path = train_xgboost(xg_data_train, xg_labels_train, xg_data_eval, xg_labels_eval, weights, 15, 45)
# xgboost_model = load_xgboost(xgboost_path)
# xg_labels_pred = xgboost_model.predict(xg_data_test)
# xg_labels_pred = xg_labels_pred.tolist()
#
# count0 = 0
# count1 = 0
# for i in range(len(xg_labels_pred)):
#     if xg_labels_pred[i] == 0 and xg_labels_test[i] == 0:
#         count0 += 1
#     if xg_labels_pred[i] == 1 and xg_labels_test[i] == 1:
#         count1 += 1
#
# accuracy0 = count0 / xg_labels_test.count(0)
# accuracy1 = count1 / xg_labels_test.count(1)
# ratio0 = xg_labels_test.count(0) / len(xg_labels_test)
# ratio1 = xg_labels_test.count(1) / len(xg_labels_test)
# predratio0 = xg_labels_test.count(0) / xg_labels_pred.count(0)
# predratio1 = xg_labels_test.count(1) / xg_labels_pred.count(1)
# print(f"Accuracy of sells: {accuracy0}")
# print(f"Accuracy of buys: {accuracy1}")
# print(f"Combined accuracy: {accuracy0 + accuracy1}")
# print(f"Ratio of sells: {ratio0}")
# print(f"Ratio of buys: {ratio1}")
# print(f"Ratio of real 0 labels to predicted 0 labels: {predratio0}")
# print(f"Ratio of real 1 labels to predicted 1 labels: {predratio1}")
# print(xg_labels_test.count(0))
# print(xg_labels_pred.count(0))
# print(xg_labels_test.count(1))
# print(xg_labels_pred.count(1))

xgboost_path = "xgboost_pipeline.pkl"
xgboost_model = load_xgboost(xgboost_path)

recent_df = get_historical_data(1735732800000, 1738411200000)
pca_training_df = get_historical_data(1717200000000, 1735516800000)
#1735732800000, 1738411200000 -> Jan 2025
#1648728000000, 1738324800000 -> March 2023 to now
recent_df.to_csv('recent_data.csv')
closes = recent_df['Close'].tolist()
recent_df = calculate_indicators(recent_df, pca_training_df)
recent_df = recent_df.dropna()
#
recent_states = []
recent_df_current = pd.DataFrame(columns=recent_df[hmm_features].columns)
recent_count = 0
for index, row in recent_df[hmm_features].iterrows():
    recent_count += 1
    recent_df_current = pd.concat([recent_df_current, pd.DataFrame([row.to_dict()])], ignore_index=True)
    recent_states.append(predict_states(hmm_model, recent_df_current, hmm_features, scaler)[-1])
    #keeps a window of 500 to improve computational efficiency
    if recent_count >= 1000:
        recent_df_current = recent_df_current.drop(recent_df_current.index[0])

recent_states = list(zip(*recent_states))

for i in range(hmm_comps):
    xg_features.append(f'State{i}')
    recent_df[f'State{i}'] = recent_states[i]

#Need this to make recent_df compatible with convert_data_to_windows method
recent_df['Label'] = 0
recent_df['Weights'] = 0
recent_df_windowed = convert_data_to_windows(recent_df[xg_features], window_size)

recent_df = recent_df.drop(index=recent_df.index[:window_size])
recent_df = recent_df.reset_index(drop=True)
recent_df_windowed = recent_df_windowed.reset_index(drop=True)
recent_df = pd.concat([recent_df_windowed, recent_df[agg_features]], axis=1)
recent_df.drop(['Label'], axis=1, inplace=True)
recent_df.drop(['Weights'], axis=1, inplace=True)

probabilities = xgboost_model.predict_proba(recent_df).tolist()
probability_0 = []
probability_1 = []
for probability in probabilities:
    probability_0.append(probability[0])
    probability_1.append(probability[1])

labels = xgboost_model.predict(recent_df)
for i in range(len(labels)):
    print(f"label: {labels[i]}, price: {closes[i]}")

print(f"percentile of sells: {np.percentile(probability_0, 53)}")
print(f"percentile of buys: {np.percentile(probability_1, 53)}")
trading_simulation(probabilities, closes, np.percentile(probability_1, 50), np.percentile(probability_0, 50), 1, starting_money=1500, spend_percentage=0.05, holdout_threshold=1)
notify()
#while True:
#     start_time = time.time()
#
#     recent_df = calculate_indicators(recent_df)
#     recent_df = recent_df.dropna()
#     recent_df['State'] = predict_states(hmm_model, recent_df, hmm_features, scaler)
#
#     #Need this to make recent_df compatible with convert_data_to_windows method
#     recent_df['Label'] = 0
#     recent_df_windowed = convert_data_to_windows(recent_df[xg_features], window_size)
#
#     recent_df = recent_df.drop(index=recent_df.index[:window_size])
#     recent_df = recent_df.reset_index(drop=True)
#     recent_df_windowed = recent_df_windowed.reset_index(drop=True)
#     recent_df = pd.concat([recent_df_windowed, recent_df[agg_features]], axis=1)
#     print(recent_df)
#     recent_df.drop(['Label'], axis=1, inplace=True)
#
#     labels_recent_pred = xgboost_model.predict(recent_df)
#     labels_recent_pred = labels_recent_pred.tolist()
#     recent_df['Label'] = labels_recent_pred
#     recent_df['PriceDiff'] = recent_df['Close_t-1'].diff()
#
#     trading_df = recent_df[['Close_t-1', 'Label']]
#     print(trading_df)
#
#     time.sleep(180)
#     recent_df = get_historical_data(54)