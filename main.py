import pandas as pd
import numpy as np
from hmmlearn import hmm
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import talib
from datetime import datetime
from xgboost import XGBClassifier
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from xgboost import plot_importance
from sklearn.model_selection import TimeSeriesSplit
import pickle
import requests
from requests.exceptions import ConnectionError, Timeout, TooManyRedirects
import json
import time
import urllib.parse
import hashlib
import hmac
import datetime
import pywt
import tkinter as tk
from tkinter import messagebox
import warnings

warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
#pd.set_option('display.float_format', '{:.0f}'.format)

api_key = 'IbIgJihiEgl4rEjWnOFazg7F4YVzJXVG8if3iKcGsurgspgblDN2F73XMPdUzOcH'

def load_and_preprocess_data(filepath, file=True, data=None, pca_training_data=None):
    if file:
        print(f"Loading data from {filepath}...")
        df = pd.read_csv(filepath, names=['Timestamp', 'Open', 'High', 'Low', 'Close', 'Volume'])
        df.drop(0, inplace=True)
        #Drop useless row
        df = df.astype(float)
    else:
        df = data.astype(float)

    eth_df = pd.read_csv('ETHUSDC_15m.csv', names=['Timestamp', 'Open', 'High', 'Low', 'Close', 'Volume'])
    eth_df.drop(['Open', 'Volume'], axis=1)
    eth_df.drop(0, inplace=True)
    eth_df = eth_df.astype(float)
    df['Label'] = (df['Close'].shift(-1) > df['Close']).astype(int)

    #Intramarket Difference
    period = 24
    btc_cmma = cmma(df['High'], df['Low'], df['Close'], period)
    eth_cmma = cmma(eth_df['High'], eth_df['Low'], eth_df['Close'], period)
    df['BTC-ETC_Diff'] = btc_cmma - eth_cmma

    df['Price_Diff'] = df['Close'].diff()
    df["Returns"] = df["Close"].pct_change()
    df["Volatility"] = df["Returns"].rolling(window=24).std()
    df['Price_Diff_Future'] = df['Price_Diff'].shift(-1)
    df['Price_Diff_Future_Abs'] = df['Price_Diff_Future'].abs()

    #prob(buy) - prob(sell) = prob(total)
    #if prob(total) is positive that means the buy signal is stronger, if negative that means the sell signal is stronger
    #prob(total) * price_diff

    #Replaces 0's with a number close to 0 to avoid infinity being present in Volume_Change
    #Since the label column has 0's present, we need to make sure that they are not replaced
    df['Volume'] = df['Volume'].replace(0, 0.00000000000000001)
    df["Volume_Change"] = df["Volume"].pct_change()

    # Need this because there was a weird error were the data in these columns were not classified as floats, this caused a problem with the pipeline as I'm not using a target encoder
    df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce')
    df['Returns'] = pd.to_numeric(df['Returns'], errors='coerce')

    df['OBV'] = talib.OBV(df['Close'], df['Volume'])

    df['MFV'] = (((df['Close'] - df['Low']) - (df['High'] - df['Close']) / (df['High'] - df['Low'])) * df['Volume'])
    df['MFV'] = df['MFV'].fillna(0) #need this incase nans are generated by dividing by 0
    df['A/D'] = 0
    for i in range(2, len(df)):
        currentMFV = df.loc[i, 'MFV']
        df.loc[i, 'A/D'] = df.loc[i-1, 'A/D'] + currentMFV
    df['CMF'] = df['MFV'].rolling(window=21).sum() / df['Volume'].rolling(window=21).sum()

    df['CumulativeVolume'] = 0
    for i in range(2, len(df)):
        new_volume = df.loc[i-1, 'CumulativeVolume'] + df.loc[i, 'Volume']
        df.loc[i, 'CumulativeVolume'] = new_volume
        if i % 97 == 0: #reset the cumulative volume everyday
            df.loc[i, 'CumulativeVolume'] = df.loc[i, 'Volume']

    df['VWAP'] = (((df['High'] + df['Low'] + df['Close']) / 3) * df['Volume']) / df['CumulativeVolume']

    df['LOW_EMA'] = talib.EMA(df['Close'], timeperiod=9)
    df['HIGH_EMA'] = talib.EMA(df['Close'], timeperiod=21)
    df['RSI'] = talib.RSI(df['Close'], timeperiod=14)
    df['Aroon'] = talib.AROONOSC(df['High'], df['Low'], timeperiod=14)
    df['Fast_%K'], df['Fast_%D'] = talib.STOCHF(df['High'], df['Low'], df['Close'], fastk_period=14)
    df['WILLR'] = talib.WILLR(df['High'], df['Low'], df['Close'])
    df['CCI'] = talib.CCI(df['High'], df['Low'], df['Close'])
    df['ATR'] = talib.ATR(df['High'], df['Low'], df['Close'])
    df['ADX'] = talib.ADX(df['High'], df['Low'], df['Close'])

    df['Momentum'] = df['Close'].diff()
    df['Absolute_Momentum'] = df['Momentum'].abs()
    df['Short_EMA_Momentum'] = talib.EMA(df['Momentum'], timeperiod=13)
    df['Short_EMA_Absolute'] = talib.EMA(df['Absolute_Momentum'], timeperiod=13)
    df['Double_EMA_Momentum'] = talib.EMA(df['Short_EMA_Momentum'], timeperiod=25)
    df['Double_EMA_Absolute'] = talib.EMA(df['Short_EMA_Absolute'], timeperiod=25)
    df['TSI'] = 100 * (df['Double_EMA_Momentum'] / df['Double_EMA_Absolute'])

    df['BB_Upper'], df['BB_Middle'], df['BB_Lower'] = talib.BBANDS(df['Close'], 20)
    df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']

    macd, macdsignal, macdhist = talib.MACDFIX(df['Close'])
    df['MACD'] = macd
    df['MACDSignal'] = macdsignal
    df['MACDHist'] = macdhist

    df['PP'] = (df['High'].shift(1) + df['Low'].shift(1) + df['Close'].shift(1)) / 3
    df['R1'] = 2 * df['PP'] - df['Low'].shift(1)
    df['R2'] = df['PP'] + (df['High'].shift(1) - df['Low'].shift(1))
    df['S1'] = 2 * df['PP'] - df['High'].shift(1)
    df['S2'] = df['PP'] - (df['High'].shift(1) - df['Low'].shift(1))

    #Ichimoku Cloud
    df['Tenkan_Sen'] = (df['High'].rolling(window=9).max() + df['Low'].rolling(window=9).min()) / 2
    df['Kijun_Sen'] = (df['High'].rolling(window=26).max() + df['Low'].rolling(window=26).min()) / 2
    df['Senkou_Span_A'] = (df['Tenkan_Sen'] + df['Kijun_Sen']) / 2
    df['Senkou_Span_B'] = (df['High'].rolling(window=52).max() + df['Low'].rolling(window=52).min()) / 2
    df['Chikou_Span'] = df['Close'].shift(26)

    #Keltner Channels
    df['KC_Middle'] = talib.EMA(df['Close'], timeperiod=20)
    df['KC_Upper'] = df['KC_Middle'] + df['ATR'] * 2
    df['KC_Lower'] = df['KC_Middle'] - df['ATR'] * 2
    df['KC_Width'] = df['KC_Upper'] - df['KC_Lower']

    #max/min points
    close = df['Close'].values
    length = len(close)
    slope = (talib.EMA(close, timeperiod=50) - talib.EMA(close, timeperiod=10)).tolist()
    extrema_unconfirmed = [0] * length

    for i in range(1, length - 1):
        if slope[i] < 0 and slope[i - 1] >= 0:
            extrema_unconfirmed[i] = -1
        if slope[i] > 0 and slope[i - 1] <= 0:
            extrema_unconfirmed[i] = 1
    df['Extrema_Unconfirmed'] = extrema_unconfirmed

    #Candlestick patterns
    df['Body_Size'] = abs(df['Close'] - df['Open'])
    df['Upper_Wick'] = df['High'] - df[['Open', 'Close']].max(axis=1)
    df['Lower_Wick'] = df[['Open', 'Close']].min(axis=1) - df['Low']
    df['Range'] = df['High'] - df['Low']

    #Add small number to avoid division by zero
    df['Body_Ratio'] = df['Body_Size'] / (df['Range'] + 1e-9)
    df['Upper_Wick_Ratio'] = df['Upper_Wick'] / (df['Range'] + 1e-9)
    df['Lower_Wick_Ratio'] = df['Lower_Wick'] / (df['Range'] + 1e-9)

    df['Two_Crows'] = talib.CDL2CROWS(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Crows'] = talib.CDL3BLACKCROWS(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Inside'] = talib.CDL3INSIDE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Strike'] = talib.CDL3LINESTRIKE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Outside'] = talib.CDL3OUTSIDE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Stars_South'] = talib.CDL3STARSINSOUTH(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Soldiers'] = talib.CDL3WHITESOLDIERS(df['Open'], df['High'], df['Low'], df['Close'])
    df['Abandoned_Baby'] = talib.CDLABANDONEDBABY(df['Open'], df['High'], df['Low'], df['Close'])
    df['Advance_Block'] = talib.CDLADVANCEBLOCK(df['Open'], df['High'], df['Low'], df['Close'])
    df['Belt_Hold'] = talib.CDLBELTHOLD(df['Open'], df['High'], df['Low'], df['Close'])
    df['Breakaway'] = talib.CDLBREAKAWAY(df['Open'], df['High'], df['Low'], df['Close'])
    df['Closing_Marubozu'] = talib.CDLCLOSINGMARUBOZU(df['Open'], df['High'], df['Low'], df['Close'])
    df['Baby_Swallow'] = talib.CDLCONCEALBABYSWALL(df['Open'], df['High'], df['Low'], df['Close'])
    df['Counterattack'] = talib.CDLCOUNTERATTACK(df['Open'], df['High'], df['Low'], df['Close'])
    df['Dark_Cloud'] = talib.CDLDARKCLOUDCOVER(df['Open'], df['High'], df['Low'], df['Close'])
    df['Doji'] = talib.CDLDOJI(df['Open'], df['High'], df['Low'], df['Close'])
    df['Doji_Star'] = talib.CDLDOJISTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['Dragonfly_Doji'] = talib.CDLDRAGONFLYDOJI(df['Open'], df['High'], df['Low'], df['Close'])
    df['Engulfing'] = talib.CDLENGULFING(df['Open'], df['High'], df['Low'], df['Close'])
    df['Evening_Doji'] = talib.CDLEVENINGDOJISTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['Evening_Star'] = talib.CDLEVENINGSTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['Up_Down_Gap'] = talib.CDLGAPSIDESIDEWHITE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Gravestone_Doji'] = talib.CDLGRAVESTONEDOJI(df['Open'], df['High'], df['Low'], df['Close'])
    df['Hammer'] = talib.CDLHAMMER(df['Open'], df['High'], df['Low'], df['Close'])
    df['Hanging_Man'] = talib.CDLHANGINGMAN(df['Open'], df['High'], df['Low'], df['Close'])
    df['Harami'] = talib.CDLHARAMI(df['Open'], df['High'], df['Low'], df['Close'])
    df['Harami_Cross'] = talib.CDLHARAMICROSS(df['Open'], df['High'], df['Low'], df['Close'])
    df['High_Wave'] = talib.CDLHIGHWAVE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Hikkake'] = talib.CDLHIKKAKE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Modified_Hikkake'] = talib.CDLHIKKAKEMOD(df['Open'], df['High'], df['Low'], df['Close'])
    df['Homing_Pigeon'] = talib.CDLHOMINGPIGEON(df['Open'], df['High'], df['Low'], df['Close'])
    df['Identical_Crows'] = talib.CDLIDENTICAL3CROWS(df['Open'], df['High'], df['Low'], df['Close'])
    df['In_Neck'] = talib.CDLINNECK(df['Open'], df['High'], df['Low'], df['Close'])
    df['Inverted_Hammer'] = talib.CDLINVERTEDHAMMER(df['Open'], df['High'], df['Low'], df['Close'])
    df['Kicking'] = talib.CDLKICKING(df['Open'], df['High'], df['Low'], df['Close'])
    df['Kicking_Length'] = talib.CDLKICKINGBYLENGTH(df['Open'], df['High'], df['Low'], df['Close'])
    df['Ladder_Bottom'] = talib.CDLLADDERBOTTOM(df['Open'], df['High'], df['Low'], df['Close'])
    df['Long_Doji'] = talib.CDLLONGLEGGEDDOJI(df['Open'], df['High'], df['Low'], df['Close'])
    df['Long_Candle'] = talib.CDLLONGLINE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Marubozu'] = talib.CDLMARUBOZU(df['Open'], df['High'], df['Low'], df['Close'])
    df['Matching_Low'] = talib.CDLMATCHINGLOW(df['Open'], df['High'], df['Low'], df['Close'])
    df['Mat_Hold'] = talib.CDLMATHOLD(df['Open'], df['High'], df['Low'], df['Close'])
    df['Morning_Doji'] = talib.CDLMORNINGDOJISTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['Morning_Star'] = talib.CDLMORNINGSTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['On_Neck'] = talib.CDLONNECK(df['Open'], df['High'], df['Low'], df['Close'])
    df['Piercing'] = talib.CDLPIERCING(df['Open'], df['High'], df['Low'], df['Close'])
    df['Rickshaw_Man'] = talib.CDLRICKSHAWMAN(df['Open'], df['High'], df['Low'], df['Close'])
    df['Rising_Falling'] = talib.CDLRISEFALL3METHODS(df['Open'], df['High'], df['Low'], df['Close'])
    df['Separating_Lines'] = talib.CDLSEPARATINGLINES(df['Open'], df['High'], df['Low'], df['Close'])
    df['Shooting_Star'] = talib.CDLSHOOTINGSTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['Short_Candle'] = talib.CDLSHORTLINE(df['Open'], df['High'], df['Low'], df['Close'])
    df['Spinning_Top'] = talib.CDLSPINNINGTOP(df['Open'], df['High'], df['Low'], df['Close'])
    df['Stalled'] = talib.CDLSTALLEDPATTERN(df['Open'], df['High'], df['Low'], df['Close'])
    df['Stick_Sandwich'] = talib.CDLSTICKSANDWICH(df['Open'], df['High'], df['Low'], df['Close'])
    df['Takuri'] = talib.CDLTAKURI(df['Open'], df['High'], df['Low'], df['Close'])
    df['Tasuki_Gap'] = talib.CDLTASUKIGAP(df['Open'], df['High'], df['Low'], df['Close'])
    df['Thrusting'] = talib.CDLTHRUSTING(df['Open'], df['High'], df['Low'], df['Close'])
    df['Tristar'] = talib.CDLTRISTAR(df['Open'], df['High'], df['Low'], df['Close'])
    df['Unique_River'] = talib.CDLUNIQUE3RIVER(df['Open'], df['High'], df['Low'], df['Close'])
    df['Upside_Gap_Crows'] = talib.CDLUPSIDEGAP2CROWS(df['Open'], df['High'], df['Low'], df['Close'])
    df['Three_Crows'] = talib.CDL3BLACKCROWS(df['Open'], df['High'], df['Low'], df['Close'])
    df['Up_Down_3Gap'] = talib.CDLXSIDEGAP3METHODS(df['Open'], df['High'], df['Low'], df['Close'])

    #
    # #Fractal Dimension
    # df['Hurst'] = df['Close'].rolling(window=100).apply(lambda x: compute_Hc(x, kind='price')[0], raw=False)
    # df['Fractal_Dimension'] = 2 - df['Hurst']

    #calculate interaction features
    df['Volume-ATR'] = df['Volume'] / df['ATR']
    df['VWAP-ATR'] = (df['Close'] - df['VWAP']) / df['ATR']
    df['RSI-MACD'] = df['RSI'] * df['MACD']
    df['Stochastic-RSI'] = df['Fast_%K'] / df['RSI']
    df['BB-KC'] = df['BB_Width'] / df['KC_Width']
    df['Ichimoku_Overlap'] = (df['Close'] - df['Kijun_Sen']) / (df['Senkou_Span_B'] - df['Senkou_Span_A'])
    df['LOW-HIGH_EMA'] = df['LOW_EMA'] / df['HIGH_EMA']

    #Candlestick interactions
    df['Closing_Marubozu-OBV'] = df['Closing_Marubozu'] * df['OBV']
    df['Closing_Marubozu-ATR'] = df['Closing_Marubozu'] * df['ATR']
    df['Closing_Marubozu-RSI'] = df['Closing_Marubozu'] * df['RSI']
    df['Marubozu-OBV'] = df['Marubozu'] * df['OBV']
    df['Marubozu-ATR'] = df['Marubozu'] * df['ATR']
    df['Marubozu-RSI'] = df['Marubozu'] * df['RSI']
    df['Short_Candle-OBV'] = df['Short_Candle'] * df['OBV']
    df['Short_Candle-ATR'] = df['Short_Candle'] * df['ATR']
    df['Short_Candle-RSI'] = df['Short_Candle'] * df['RSI']
    df['Long_Candle-OBV'] = df['Long_Candle'] * df['OBV']
    df['Long_Candle-ATR'] = df['Long_Candle'] * df['ATR']
    df['Long_Candle-RSI'] = df['Long_Candle'] * df['RSI']
    df['Doji-OBV'] = df['Doji'] * df['OBV']
    df['Doji-ATR'] = df['Doji'] * df['ATR']
    df['Doji-RSI'] = df['Doji'] * df['RSI']
    df['Long_Doji-OBV'] = df['Long_Doji'] * df['OBV']
    df['Long_Doji-ATR'] = df['Long_Doji'] * df['ATR']
    df['Long_Doji-RSI'] = df['Long_Doji'] * df['RSI']
    df['Dragonfly_Doji-OBV'] = df['Dragonfly_Doji'] * df['OBV']
    df['Dragonfly_Doji-ATR'] = df['Dragonfly_Doji'] * df['ATR']
    df['Dragonfly_Doji-RSI'] = df['Dragonfly_Doji'] * df['RSI']
    df['Belt_Hold-OBV'] = df['Belt_Hold'] * df['OBV']
    df['Belt_Hold-ATR'] = df['Belt_Hold'] * df['ATR']
    df['Belt_Hold-RSI'] = df['Belt_Hold'] * df['RSI']

    #Donchain Breakout
    df['Upper'] = df['Close'].rolling(288 - 1).max().shift(1)
    df['Lower'] = df['Close'].rolling(288 - 1).min().shift(1)
    df['Don_Signal'] = np.nan
    df.loc[df['Close'] > df['Upper'], 'Don_Signal'] = 1
    df.loc[df['Close'] < df['Lower'], 'Don_Signal'] = -1
    df['Don_Signal'] = df['Don_Signal'].ffill()
    df.drop(['Upper', 'Lower'], axis=1, inplace=True)

    # Generate Scree Plot
    # explained_variance_ratio = pca.explained_variance_ratio_
    # components = range(1, len(explained_variance_ratio) + 1)
    #
    # plt.figure(figsize=(8, 5))
    # plt.bar(components, explained_variance_ratio, color='blue', alpha=0.7, label='Explained Variance Ratio')
    # plt.plot(components, explained_variance_ratio, 'ro-', label='Cumulative Explained Variance')
    # plt.xlabel('Principal Components')
    # plt.ylabel('Explained Variance Ratio')
    # plt.title('Scree Plot for RSI PCA Analysis')
    # plt.legend()
    # plt.show()

    #drop temporary columns
    df.drop(['CumulativeVolume', 'Absolute_Momentum', 'Short_EMA_Momentum', 'Short_EMA_Absolute', 'Double_EMA_Momentum', 'Double_EMA_Absolute'], axis=1, inplace=True)
    df.replace([np.inf, -np.inf], 0, inplace=True)
    df.fillna(0, inplace=True)

    agg_excluded = ['Timestamp', 'Label', 'Price_Diff_Future_Abs', 'Price_Diff_Future']
    agg_period = [4, 16, 48, 96, 384, 1152] #1h, 4h, 12h, 1d, 4d, 12d
    agg_df = pd.DataFrame()
    for period in agg_period:
        for feature in df.columns:
            if feature not in agg_excluded:
                agg_df[f'{feature}_Mean_{period}'] = df[feature].rolling(window=period).mean()
                agg_df[f'{feature}_Dev_{period}'] = df[feature].rolling(window=period).std()
                agg_df[f'{feature}_Drawdown_{period}'] = (df[feature] / df[feature].rolling(window=period).max() - 1).rolling(window=period).min()

    df['Label_Mean_4'] = df['Label'].shift(1).rolling(window=4).mean()
    df['Label_Mean_16'] = df['Label'].shift(1).rolling(window=16).mean()
    df['Label_Mean_48'] = df['Label'].shift(1).rolling(window=48).mean()
    df['Label_Mean_96'] = df['Label'].shift(1).rolling(window=96).mean()
    df['Label_Mean_384'] = df['Label'].shift(1).rolling(window=384).mean()
    df['Label_Mean_1152'] = df['Label'].shift(1).rolling(window=1152).mean()
    df['Label_Dev_4'] = df['Label'].shift(1).rolling(window=4).std()
    df['Label_Dev_16'] = df['Label'].shift(1).rolling(window=16).std()
    df['Label_Dev_48'] = df['Label'].shift(1).rolling(window=48).std()
    df['Label_Dev_96'] = df['Label'].shift(1).rolling(window=96).std()
    df['Label_Dev_384'] = df['Label'].shift(1).rolling(window=384).std()
    df['Label_Dev_1152'] = df['Label'].shift(1).rolling(window=1152).std()
    df['Label_Drawdown_4'] = (df['Label'].shift(1) / df['Label'].shift(1).rolling(window=4).max() - 1).rolling(window=4).min()
    df['Label_Drawdown_16'] = (df['Label'].shift(1) / df['Label'].shift(1).rolling(window=16).max() - 1).rolling(window=16).min()
    df['Label_Drawdown_48'] = (df['Label'].shift(1) / df['Label'].shift(1).rolling(window=48).max() - 1).rolling(window=48).min()
    df['Label_Drawdown_96'] = (df['Label'].shift(1) / df['Label'].shift(1).rolling(window=96).max() - 1).rolling(window=96).min()
    df['Label_Drawdown_384'] = (df['Label'].shift(1) / df['Label'].shift(1).rolling(window=384).max() - 1).rolling(window=384).min()
    df['Label_Drawdown_1152'] = (df['Label'].shift(1) / df['Label'].shift(1).rolling(window=1152).max() - 1).rolling(window=1152).min()

    df = pd.concat([df, agg_df], axis=1)
    df = df.dropna(axis=1, thresh=len(df) - 1452)
    df = df.fillna(0)

    #defragmenting dataframe
    df = df.copy()

    if file:
        training_df = df[df['Timestamp'] > 1654056000000] #January 1st 2023 12:15 am to present
        pca_training_df = df[((df['Timestamp'] >= 1641013200000) & (df['Timestamp'] <= 1654056000000))] #Jan 1st 12am 2022 to June 1st 12am 2022
    else:
        training_df = df
        pca_training_df = pca_training_data.astype(float)

    training_df.reset_index(drop=True, inplace=True)
    pca_training_df.reset_index(drop=True, inplace=True)

    # RSI PCA analysis
    rsis = pd.DataFrame()
    for p in range(2, 33):
        rsis[p] = talib.RSI(pca_training_df['Close'], timeperiod=p)

    rsi_means = rsis.mean()
    rsis -= rsi_means
    rsis = rsis.dropna()

    rsi_pca = PCA(n_components=3, random_state=42)
    rsi_pca.fit(rsis)

    training_rsis = pd.DataFrame()
    for p in range(2, 33):
        training_rsis[p] = talib.RSI(training_df['Close'], timeperiod=p)

    training_rsi_means = training_rsis.mean()
    training_rsis -= training_rsi_means
    training_rsis = training_rsis.dropna()

    pca_data = rsi_pca.transform(training_rsis)
    rsi_pca_df = pd.DataFrame(pca_data, columns=['RSI_PC1', 'RSI_PC2', 'RSI_PC3'])
    training_df = pd.concat([training_df, rsi_pca_df], axis=1)

    # ATR PCA analysis
    atrs = pd.DataFrame()
    for p in range(2, 33):
        atrs[p] = talib.ATR(pca_training_df['High'], pca_training_df['Low'], pca_training_df['Close'], timeperiod=p)

    atr_means = atrs.mean()
    atrs -= atr_means
    atrs = atrs.dropna()

    atr_pca = PCA(n_components=3, random_state=42)
    atr_pca.fit(atrs)

    training_atrs = pd.DataFrame()
    for p in range(2, 33):
        training_atrs[p] = talib.ATR(training_df['High'], training_df['Low'], training_df['Close'], timeperiod=p)

    training_atr_means = training_atrs.mean()
    training_atrs -= training_atr_means
    training_atrs = training_atrs.dropna()

    pca_data = atr_pca.transform(training_atrs)
    atr_pca_df = pd.DataFrame(pca_data, columns=['ATR_PC1', 'ATR_PC2', 'ATR_PC3'])
    training_df = pd.concat([training_df, atr_pca_df], axis=1)

    # ADX PCA analysis
    adxs = pd.DataFrame()
    for p in range(2, 33):
        adxs[p] = talib.ADX(pca_training_df['High'], pca_training_df['Low'], pca_training_df['Close'], timeperiod=p)

    adx_means = adxs.mean()
    adxs -= adx_means
    adxs = adxs.dropna()

    adx_pca = PCA(n_components=5, random_state=42)
    adx_pca.fit(adxs)

    training_adxs = pd.DataFrame()
    for p in range(2, 33):
        training_adxs[p] = talib.ADX(training_df['High'], training_df['Low'], training_df['Close'], timeperiod=p)

    training_adx_means = training_adxs.mean()
    training_adxs -= training_adx_means
    training_adxs = training_adxs.dropna()

    pca_data = adx_pca.transform(training_adxs)
    adx_pca_df = pd.DataFrame(pca_data, columns=['ADX_PC1', 'ADX_PC2', 'ADX_PC3', 'ADX_PC4', 'ADX_PC5'])
    training_df = pd.concat([training_df, adx_pca_df], axis=1)

    training_df.fillna(0, inplace=True)
    #calculate aggregate pca features
    agg_included = ['RSI_PC1', 'RSI_PC2', 'RSI_PC3', 'ATR_PC1', 'ATR_PC2', 'ATR_PC3', 'ADX_PC1', 'ADX_PC2', 'ADX_PC3', 'ADX_PC4', 'ADX_PC5']
    agg_period = [4, 16, 48, 96, 384, 1152]  # 1h, 4h, 12h, 1d, 4d, 12d
    training_agg_df = pd.DataFrame()
    for period in agg_period:
        #print()
        for feature in training_df.columns:
            if feature in agg_included:
                #print(f"'{feature}_Mean_{period}', ", end=' ')
                training_agg_df[f'{feature}_Mean_{period}'] = training_df[feature].rolling(window=period).mean()
                training_agg_df[f'{feature}_Dev_{period}'] = training_df[feature].rolling(window=period).std()
                training_agg_df[f'{feature}_Drawdown_{period}'] = (training_df[feature] / training_df[feature].rolling(window=period).max() - 1).rolling(window=period).min()

    training_df = pd.concat([training_df, training_agg_df], axis=1)
    training_df = training_df.dropna(axis=1, thresh=len(training_df) - 1452)
    training_df = training_df.fillna(0)

    ms = MinMaxScaler(feature_range=(0, 1))
    training_df['Weights'] = ms.fit_transform(training_df[['Price_Diff_Future_Abs']]) * 10
    df.drop(['Price_Diff_Future_Abs', 'Price_Diff_Future'], axis=1, inplace=True)
    training_df = training_df.copy()

    return training_df

def cmma(High, Low, Close, period, atr_period = 168):
    atr = talib.ATR(High, Low, Close, atr_period)
    ma = Close.rolling(period).mean()
    ind = (Close - ma) / (atr * period ** 0.5)
    return ind

def train_hmm(data, features, scaler, n_components=3):
    X = data[features].values
    X_scaled = scaler.fit_transform(X)
    model = hmm.GaussianHMM(n_components=n_components, covariance_type='full', n_iter=1000, random_state=42, verbose=True)
    model.fit(X_scaled)

    filepath = "hmm_model.pkl"
    with open("hmm_model.pkl", "wb") as f:
        pickle.dump(model, f)

    return filepath

def load_hmm(filepath):
    with open(filepath, "rb") as f:
        model = pickle.load(f)
    return model

def predict_states(model, data, features, scaler):
     X = data[features].values
     X_scaled = scaler.transform(X)
     states = model.predict_proba(X_scaled)
     return states

def custom_pnl_objective(y_true, y_pred):
    y_true[y_true == 0] = -1
    p_buy = 1.0 / (1.0 + np.exp(-y_pred))
    prob_diff = 2.0 * p_buy - 1.0
    loss = -(prob_diff * y_true)

    grad = -((2 * np.exp(-y_pred) * y_true) / ((np.exp(-y_pred) + 1) ** 2))
    hess = -((2 * np.exp(-2 * y_pred) * y_true * (-np.exp(y_pred) + 1)) / (np.exp(-y_pred) + 1) ** 3)
    hess = np.maximum(hess, 1e-6)

    #print(f"y_true:{y_true}, y_pred:{y_pred}, p_buy:{p_buy}, prob_diff:{prob_diff}, grad:{grad}, hess:{hess}")
    #print(f"grad counts:{np.unique(grad, return_counts=True)}, true counts:{np.unique(y_true, return_counts=True)}, pred counts: {np.unique(y_pred, return_counts=True)}")

    return grad.astype(np.float32), hess.astype(np.float32)

# def train_xgboost(data_train, labels_train, data_test, labels_test, weights, splits, iters):
#     tscv = TimeSeriesSplit(n_splits=splits)
#     scale_weight = labels_train.count(1)/labels_train.count(0)
#     print(scale_weight)
#
#     pipeline = Pipeline(steps=[('xgb', XGBClassifier(random_state=42, eval_metric='auc', early_stopping_rounds=70, tree_method='hist', device='cuda', objective='binary:logistic'))])
#
#     search_space = {
#         'xgb__max_depth': Integer(3, 14),
#         'xgb__learning_rate': Real(0.0005, 1.0, prior='log-uniform'),
#         'xgb__subsample': Real(0.15, 1.0),
#         'xgb__colsample_bytree': Real(0.5, 1.0),
#         'xgb__colsample_bylevel': Real(0.5, 1.0),
#         'xgb__colsample_bynode': Real(0.5, 1.0),
#         'xgb__reg_alpha': Real(0.0, 8.0),
#         'xgb__reg_lambda': Real(0.0, 13.0),
#         'xgb__gamma': Real(0.0, 12.0),
#         'xgb__n_estimators': Integer(100, 5000),
#         'xgb__min_child_weight': Integer(1, 15),
#         'xgb__max_bin': Integer(128, 512),
#         'xgb__grow_policy': Categorical(["depthwise", "lossguide"]),
#         'xgb__max_delta_step': Integer(0, 10),
#         'xgb__scale_pos_weight': Real(1.04, 1.08),
#         'xgb__booster': Categorical(["gbtree", "dart"])
#     }
#     opt = BayesSearchCV(pipeline, search_space, cv=tscv, n_iter=iters, scoring='roc_auc', random_state=42)
#
#     opt.fit(data_train, labels_train, xgb__eval_set=[(data_test, labels_test)], xgb__sample_weight=weights)
#     print("Best estimator: ", opt.best_estimator_)
#     print("Best score: ", opt.best_score_)
#     print("Best params: ", opt.best_params_)
#     print(opt.score(data_test, labels_test))
#     labels_pred = opt.predict(data_test)
#     labels_pred = labels_pred.tolist()
#     print(labels_pred.count(0))
#     print(labels_pred.count(1))
#     print(opt.best_estimator_.steps)
#
#     xgboost_step = opt.best_estimator_.steps[0]
#     xgboost_model = xgboost_step[1]
#     plot_importance(xgboost_model, max_num_features=100)
#     plt.show()
#
#     # save the model
#     filepath = "xgboost_pipeline.pkl"
#     with open(filepath, "wb") as f:
#         pickle.dump(opt.best_estimator_, f)
#
#     return filepath

def train_xgboost(data_train, labels_train, data_test, labels_test, weights, splits, iters):
    params = {
        'max_depth': 10,
        'learning_rate': 0.0072011999078535486,
        'subsample': 0.7168045624622187,
        'colsample_bytree': 0.6578997967435244,
        'colsample_bylevel': 0.840082181391577,
        'colsample_bynode': 0.966433999423917,
        'reg_alpha': 1.424671347168512,
        'reg_lambda': 8.217243165955011,
        'gamma': 8.041775379227216,
        'n_estimators': 3500,
        'min_child_weight': 9,
        'max_bin': 412,
        'grow_policy': "depthwise",
        'max_delta_step': 3,
        'scale_pos_weight': 1.051,
        'booster': 'gbtree'
    }

    model = XGBClassifier(
        random_state=42,
        eval_metric='auc',
        early_stopping_rounds=100,
        tree_method='hist',
        device='cuda',
        objective='binary:logistic',
        **params
    )

    model.fit(
        data_train,
        labels_train,
        eval_set=[(data_test, labels_test)],
        sample_weight=weights
    )

    plot_importance(model, max_num_features=100)
    plt.show()

    filepath = "xgboost_pipeline.pkl"
    with open(filepath, "wb") as f:
        pickle.dump(model, f)

    return filepath

def load_xgboost(filepath):
    with open(filepath, "rb") as f:
        model = pickle.load(f)
    return model

def get_binanceus_signature(data, secret):
    postdata = urllib.parse.urlencode(data)
    message = postdata.encode()
    byte_key = bytes(secret, 'UTF-8')
    mac = hmac.new(byte_key, message, hashlib.sha256).hexdigest()
    return mac

def exchange_btc(side, quoteOrderQty):
    data = {
        "symbol": 'BTCUSDC',
        "side": side,
        "type": 'MARKET',
        "quoteOrderQty": quoteOrderQty,
        "timestamp": int(round(time.time() * 1000))
    }
    headers = {
        'X-MBX-APIKEY': api_key
    }
    signature = get_binanceus_signature(data, api_sec)
    payload = {
        **data,
        "signature": signature,
    }
    req = requests.post(('https://api.binance.us/api/v3/order'), headers=headers, data=payload)
    return req.text

def get_historical_data(start_time, end_time, symbol='BTCUSDC'):
    url = 'https://api.binance.us/api/v3/klines'
    headers = {
        'X-MBX-APIKEY': api_key,
    }
    parameters = {
        'symbol': symbol,
        'interval': '15m',
        'startTime': str(start_time),
        'limit': '1000'
    }

    flag = True
    dataframes = []
    session = requests.Session()
    while(flag):
        session.headers.update(headers)

        try:
            response = session.get(url, params=parameters)
            data = json.loads(response.text)
        except (ConnectionError, requests.Timeout, requests.TooManyRedirects) as e:
            print(e)

        df = pd.DataFrame(columns=["Timestamp", "Open", "High", "Low", "Close", "Volume"])

        for i in range(len(data)):
            df.loc[len(df)] = {"Timestamp": int(data[i][0]),
                               "Open": float(data[i][1]),
                               "High": float(data[i][2]),
                               "Low": float(data[i][3]),
                               "Close": float(data[i][4]),
                               "Volume": float(data[i][5])}
            close_time = int(data[i][6])

            if close_time >= end_time:
                flag = False
        dataframes.append(df)

        start_time += (1000*15*60000) #add time in milliseconds
        parameters['startTime'] = start_time #update time

    combined_df = pd.concat(dataframes, ignore_index=True)
    return combined_df

def get_recent_data(count='1000', symbol='BTCUSDC'):
    url = 'https://api.binance.us/api/v3/klines'
    headers = {
        'X-MBX-APIKEY': api_key,
    }
    parameters = {
        'symbol': symbol,
        'interval': '15m',
        'limit': count
    }

    session = requests.Session()
    session.headers.update(headers)

    try:
        response = session.get(url, params=parameters)
        data = json.loads(response.text)
    except (ConnectionError, requests.Timeout, requests.TooManyRedirects) as e:
        print(e)

    df = pd.DataFrame(columns=["Timestamp", "Open", "High", "Low", "Close", "Volume"])

    for i in range(len(data)):
        df.loc[len(df)] = {"Timestamp": int(data[i][0]),
                            "Open": float(data[i][1]),
                            "High": float(data[i][2]),
                            "Low": float(data[i][3]),
                            "Close": float(data[i][4]),
                            "Volume": float(data[i][5])}

    return df

def convert_data_to_windows(data, window_size=2):
    final = pd.DataFrame()
    non_windowed = ['Label', 'Weights', 'RSI_PC1', 'RSI_PC2', 'RSI_PC3', 'ATR_PC1', 'ATR_PC2', 'ATR_PC3', 'ADX_PC1', 'ADX_PC2', 'ADX_PC3', 'ADX_PC4', 'ADX_PC5']
    for i in range(window_size):
        for feature in data.columns:
            if feature not in non_windowed:
                final[f'{feature}_t{i}'] = data[feature].shift(i)

    for feature in non_windowed:
        final[feature] = data[feature]
    final.dropna(inplace=True)
    return final
    # window_data['Label'] = window_data['Label'].shift(1)
    # window_data['Weights'] = window_data['Weights'].shift(1)

def trading_simulation(labels, closes, starting_money=500, spend_percentage=0.1):
    money = starting_money
    bitcoin = 0
    close_prices = closes
    buy_order = False
    sell_order = False

    #0 is sell, 2 is buy
    count = 0
    day_count = 0
    overconfidence = 0
    last_bought_price = 0
    last_sold_price = 0
    profitable_trade_count = 0
    trade_count = 0
    historical_daily_starting_assets = []
    for label in labels:
        print(f"close: {close_prices[count]}, label: {label}")
        if count % 96 == 0: #update daily starting assets at the start of each day
            daily_starting_assets = money + (bitcoin * close_prices[count])
            historical_daily_starting_assets.append(daily_starting_assets)
            # if len(historical_daily_starting_assets) >= 2:
            #     if (historical_daily_starting_assets[day_count - 1] * 0.95) > historical_daily_starting_assets[day_count]:
            #         print(f"Stop-loss triggered on Day {day_count}: more than 5% drop in a single day.")
            #         print(f"Current money: {money}, Current bitcoin: {bitcoin}, Current count: {count}")
            #         return
            day_count += 1

        # hold = False
        # if (probability[0] >= (0.5 - holdout_threshold) and (probability[0] <= (0.5 - holdout_threshold))) and (probability[1] >= (0.5 - holdout_threshold) and (probability[1] <= (0.5 - holdout_threshold))):
        #     hold = True

        if label == 1 and sell_order == False:
            sell_order = True
        elif label == 0 and buy_order == False:
            #print(f"Overconfidence on Day {count}: {overconfidence}")
            buy_order = True

        #print(hold)
        #print(buy_sell)
        if sell_order and label == 0:
            money += bitcoin * close_prices[count]
            last_sold_price = close_prices[count]
            print(f"Day {day_count}: SOLD {bitcoin} BTC at {last_sold_price} each.")
            bitcoin = 0
            sell_order = False
        elif buy_order and label == 1:
            amount_to_spend = money * spend_percentage
            bitcoin_bought = amount_to_spend / close_prices[count]
            bitcoin += bitcoin_bought
            money -= amount_to_spend
            buy_order = False
            last_bought_price = close_prices[count]
            print(f"Day {day_count}: BOUGHT {bitcoin_bought} BTC at {last_bought_price} each.")

        if last_sold_price > last_bought_price:
            profitable_trade_count += 1
            trade_count += 1
        else:
            trade_count += 1

        print(f"Money: {money}, Bitcoin: {bitcoin}, Count: {count}")
        count += 1

    total_assets = money + bitcoin * close_prices[count-2]
    print("Final money: ", total_assets)
    print("Profit: ", total_assets - starting_money)
    print("Percentage profitable: ", profitable_trade_count / trade_count)

def walk_forward_trading(model, data, starting_money=500, spend_percentage=0.1):
    money = starting_money
    bitcoin = 0
    buy_order = False
    sell_order = False
    last_bought_price = 0
    last_sold_price = 0
    profitable_trade_count = 0
    trade_count = 0
    for i in range(len(data)):
        if i > 1:
            current_split = data.iloc[:i]
            labels = model.predict(current_split)
            probas = model.predict_proba(current_split).tolist()
            print(probas[-1])
            label = labels[-1]
            close = current_split['Close_t0'].iloc[-1]

            if label == 1 and sell_order == False:
                sell_order = True
            elif label == 0 and buy_order == False:
                # print(f"Overconfidence on Day {count}: {overconfidence}")
                buy_order = True

            # print(hold)
            # print(buy_sell)
            if sell_order and label == 0:
                money += bitcoin * close
                last_sold_price = close
                print(f"Timestep {i}: SOLD {bitcoin} BTC at {last_sold_price} each.")
                bitcoin = 0
                sell_order = False
            elif buy_order and label == 1:
                amount_to_spend = money * spend_percentage
                bitcoin_bought = amount_to_spend / close
                bitcoin += bitcoin_bought
                money -= amount_to_spend
                buy_order = False
                last_bought_price = close
                print(f"Timestep {i}: BOUGHT {bitcoin_bought} BTC at {last_bought_price} each.")

            if last_sold_price > last_bought_price:
                profitable_trade_count += 1
                trade_count += 1
            else:
                trade_count += 1

            print(f"Money: {money}, Bitcoin: {bitcoin}, Timestep: {i}")

    total_assets = money + bitcoin * close
    print("Final money: ", total_assets)
    print("Profit: ", total_assets - starting_money)
    print("Percentage profitable: ", profitable_trade_count / trade_count)

def get_next_interval(interval_seconds):
    now = time.time()
    next_interval = ((now // interval_seconds) + 1) * interval_seconds
    return next_interval - now

def notify():
    root = tk.Tk()
    root.withdraw()
    messagebox.showinfo("Notification", "Your code has finished running!")
    root.destroy()

def walk_forward(data, xg_features, agg_features, n_splits=5, test_size=0.15, val_size=0.15, window_size=2):
    splits = []
    tscv = TimeSeriesSplit(n_splits=n_splits)

    for train_idx, test_val_idx in tscv.split(data):
        n_test_val = len(test_val_idx)
        n_val = int(n_test_val * (val_size / (test_size + val_size)))

        val_idx = test_val_idx[:n_val]
        test_idx = test_val_idx[n_val:]

        splits.append((train_idx, val_idx, test_idx))

    split_count = 0
    for split in splits:
        split_count += 1
        print("Split: " + str(split_count))

        data_train = data.iloc[min(split[0]):max(split[0])]
        data_eval = data.iloc[min(split[1]):max(split[1])]
        data_test = data.iloc[min(split[2]):max(split[2])]

        # Build xgboost model
        xg_data_eval = convert_data_to_windows(data_eval[xg_features], window_size)
        xg_data_train = convert_data_to_windows(data_train[xg_features], window_size)
        xg_data_test = convert_data_to_windows(data_test[xg_features], window_size)

        # dropping the first few rows to line up data with the xgboost data
        data_train = data_train.drop(index=data_train.index[:window_size])
        data_eval = data_eval.drop(index=data_eval.index[:window_size])
        data_test = data_test.drop(index=data_test.index[:window_size])

        # need to reset index so that the concat works properly
        xg_data_train = xg_data_train.reset_index(drop=True)
        data_train = data_train.reset_index(drop=True)
        xg_data_eval = xg_data_eval.reset_index(drop=True)
        data_eval = data_eval.reset_index(drop=True)
        xg_data_test = xg_data_test.reset_index(drop=True)
        data_test = data_test.reset_index(drop=True)
        xg_data_train = pd.concat([xg_data_train, data_train[agg_features]], axis=1)
        xg_data_eval = pd.concat([xg_data_eval, data_eval[agg_features]], axis=1)
        xg_data_test = pd.concat([xg_data_test, data_test[agg_features]], axis=1)

        xg_data_train.dropna(inplace=True)
        xg_data_eval.dropna(inplace=True)
        xg_data_test.dropna(inplace=True)

        print(xg_data_train[['Close_t0', 'Weights', 'Label']])
        xg_labels_train = xg_data_train.pop('Label').tolist()
        xg_labels_train = [int(x) for x in xg_labels_train]
        xg_labels_eval = xg_data_eval.pop('Label').tolist()
        xg_labels_eval = [int(x) for x in xg_labels_eval]
        xg_labels_test = xg_data_test.pop('Label').tolist()
        xg_labels_test = [int(x) for x in xg_labels_test]

        weights = xg_data_train['Weights'].tolist()
        xg_data_train.drop(columns='Weights', inplace=True)
        xg_data_eval.drop(columns='Weights', inplace=True)
        xg_data_test.drop(columns='Weights', inplace=True)
        xgboost_path = train_xgboost(xg_data_train, xg_labels_train, xg_data_eval, xg_labels_eval, weights, 5, 5)
        xgboost_model = load_xgboost(xgboost_path)
        xg_labels_pred = xgboost_model.predict(xg_data_test)
        xg_labels_pred = xg_labels_pred.tolist()

        count0 = 0
        count1 = 0
        for i in range(len(xg_labels_pred)):
            if xg_labels_pred[i] == 0 and xg_labels_test[i] == 0:
                count0 += 1
            if xg_labels_pred[i] == 1 and xg_labels_test[i] == 1:
                count1 += 1

        accuracy0 = count0 / xg_labels_test.count(0)
        accuracy1 = count1 / xg_labels_test.count(1)
        ratio0 = xg_labels_test.count(0) / len(xg_labels_test)
        ratio1 = xg_labels_test.count(1) / len(xg_labels_test)
        predratio0 = xg_labels_test.count(0) / xg_labels_pred.count(0)
        predratio1 = xg_labels_test.count(1) / xg_labels_pred.count(1)
        print(f"Accuracy of sells: {accuracy0}")
        print(f"Accuracy of buys: {accuracy1}")
        print(f"Combined accuracy: {accuracy0 + accuracy1}")
        print(f"Ratio of sells: {ratio0}")
        print(f"Ratio of buys: {ratio1}")
        print(f"Ratio of real 0 labels to predicted 0 labels: {predratio0}")
        print(f"Ratio of real 1 labels to predicted 1 labels: {predratio1}")
    return xgboost_model

def continuous_sim(xgboost_model, xg_features, agg_features):
    twenty_days_ms = 60 * 24 * 60 * 60 * 1000
    pca_training_df = get_historical_data(1735707600000,  	1738386000000)

    current_time = int(time.time() * 1000)
    # while current_time % (15 * 60 * 1000) != 0:
    #     current_time = int(time.time() * 1000)

    print(f"Current time: {current_time}")
    start_time = current_time - twenty_days_ms
    raw_data = get_historical_data(start_time, current_time)
    recent_df = raw_data.copy()
    # 1735732800000, 1738411200000 -> Jan 2025
    # 1648728000000, 1738324800000 -> March 2023 to now
    recent_df = load_and_preprocess_data("", False, data=recent_df, pca_training_data=pca_training_df)
    #recent_df = recent_df.dropna()

    recent_df['Label'] = 0
    recent_df['Weights'] = 0
    recent_df_windowed = convert_data_to_windows(recent_df[xg_features], window_size)

    recent_df = recent_df.drop(index=recent_df.index[:window_size])
    recent_df = recent_df.reset_index(drop=True)
    recent_df_windowed = recent_df_windowed.reset_index(drop=True)
    recent_df = pd.concat([recent_df_windowed, recent_df[agg_features]], axis=1)
    recent_df.drop(['Label'], axis=1, inplace=True)
    recent_df.drop(['Weights'], axis=1, inplace=True)
    recent_df = recent_df.loc[recent_df['Close_t0'] != 0]
    pred_labels = xgboost_model.predict(recent_df).tolist()
    last_label = pred_labels[-1]

    time.sleep(900)

    while True:
        row = get_recent_data('1')
        raw_data = pd.concat([raw_data, row], ignore_index=True)
        recent_df = raw_data.copy()
        # 1735732800000, 1738411200000 -> Jan 2025
        # 1648728000000, 1738324800000 -> March 2023 to now
        recent_df = load_and_preprocess_data("", False, data=recent_df, pca_training_data=pca_training_df)
        recent_df = recent_df.dropna()

        recent_df['Label'] = 0
        recent_df['Weights'] = 0
        recent_df_windowed = convert_data_to_windows(recent_df[xg_features], window_size)

        recent_df = recent_df.drop(index=recent_df.index[:window_size])
        recent_df = recent_df.reset_index(drop=True)
        recent_df_windowed = recent_df_windowed.reset_index(drop=True)
        recent_df = pd.concat([recent_df_windowed, recent_df[agg_features]], axis=1)
        recent_df.drop(['Label'], axis=1, inplace=True)
        recent_df.drop(['Weights'], axis=1, inplace=True)

        # 1740362400000

        last_price = recent_df['Close_t0'].iloc[-2]
        recent_price = recent_df['Close_t0'].iloc[-1]
        last_timestamp = recent_df['Timestamp_t0'].iloc[-2]
        recent_timestamp = recent_df['Timestamp_t0'].iloc[-1]
        print(f"Last price: {last_price} at time {last_timestamp}")
        print(f"Recent price: {recent_price} at time {recent_timestamp}")
        print("Label: " + str(last_label))
        if last_price >= recent_price and last_label == 0:
            print("Sell prediction correct")
        elif last_price < recent_price and last_label == 1:
            print("Buy prediction correct")
        else:
            print("Prediction incorrect")

        pred_labels = xgboost_model.predict(recent_df).tolist()
        probas = xgboost_model.predict_proba(recent_df).tolist()
        print(probas[-1])
        last_label = pred_labels[-1]
        time.sleep(900)

window_size = 2
data = load_and_preprocess_data("BTCUSDC_15m.csv")
scaler = StandardScaler()
hmm_features = ['Close', 'BTC-ETC_Diff', 'Price_Diff', 'Returns', 'Volatility', 'Don_Signal', 'RSI', 'Upper_Wick_Ratio', 'Volume_Change', 'Body_Ratio', 'Lower_Wick_Ratio', 'Upper_Wick', 'Body_Size', 'VWAP', 'Lower_Wick', 'Fast_%K',
                'Volume-ATR', 'CCI', 'Volume', 'BB-KC', 'Range', 'MACDHist', 'MACDSignal', 'MACD', 'Stochastic-RSI', 'RSI_Mean_4', 'Chikou_Span_Dev_48', 'Chikou_Span_Drawdown_4', 'KC_Upper_Drawdown_4']
xg_features = ['Close', 'RSI_PC1', 'RSI_PC2', 'RSI_PC3', 'ATR_PC1', 'ATR_PC2', 'ATR_PC3', 'ADX_PC1', 'ADX_PC2', 'ADX_PC3', 'ADX_PC4', 'ADX_PC5', 'Stochastic-RSI', 'Fast_%K', 'Volatility', 'BTC-ETC_Diff', 'Returns',
               'Upper_Wick_Ratio', 'Range', 'Upper_Wick', 'MACDSignal', 'LOW-HIGH_EMA', 'Ichimoku_Overlap', 'RSI-MACD', 'Body_Ratio', 'MACD', 'Price_Diff', 'Timestamp', 'VWAP-ATR', 'WILLR',
               'RSI', 'Doji-OBV', 'BB-KC', 'Lower_Wick', 'Body_Size', 'MACDHist', 'BB_Width', 'ATR', 'VWAP', 'Volume-ATR', 'Lower_Wick_Ratio', 'KC_Width', 'TSI', 'Momentum', 'ADX', 'CCI', 'Fast_%D',
               'CMF', 'OBV', 'Volume_Change', 'Volume', 'Weights', 'Label']
agg_features = ['Stochastic-RSI_Mean_4', 'BTC-ETC_Diff_Mean_4', 'Price_Diff_Mean_4', 'Returns_Mean_4', 'Volatility_Mean_4', 'Volume_Change_Mean_4', 'OBV_Mean_4', 'CMF_Mean_4', 'VWAP_Mean_4', 'RSI_Mean_4', 'Fast_%K_Mean_4', 'Fast_%D_Mean_4', 'WILLR_Mean_4', 'CCI_Mean_4', 'ATR_Mean_4', 'ADX_Mean_4', 'Momentum_Mean_4', 'TSI_Mean_4', 'BB_Width_Mean_4', 'MACD_Mean_4', 'MACDSignal_Mean_4', 'MACDHist_Mean_4', 'KC_Width_Mean_4', 'Body_Size_Mean_4', 'Upper_Wick_Mean_4', 'Lower_Wick_Mean_4', 'Range_Mean_4', 'Body_Ratio_Mean_4', 'Upper_Wick_Ratio_Mean_4', 'Lower_Wick_Ratio_Mean_4', 'Doji-OBV_Mean_4', 'Volume-ATR_Mean_4', 'VWAP-ATR_Mean_4', 'RSI-MACD_Mean_4', 'BB-KC_Mean_4', 'Ichimoku_Overlap_Mean_4', 'LOW-HIGH_EMA_Mean_4',
                'Stochastic-RSI_Mean_16', 'BTC-ETC_Diff_Mean_16', 'Price_Diff_Mean_16', 'Returns_Mean_16', 'Volatility_Mean_16', 'Volume_Change_Mean_16', 'OBV_Mean_16', 'CMF_Mean_16', 'VWAP_Mean_16', 'RSI_Mean_16', 'Fast_%K_Mean_16', 'Fast_%D_Mean_16', 'WILLR_Mean_16', 'CCI_Mean_16', 'ATR_Mean_16', 'ADX_Mean_16', 'Momentum_Mean_16', 'TSI_Mean_16', 'BB_Width_Mean_16', 'MACD_Mean_16', 'MACDSignal_Mean_16', 'MACDHist_Mean_16', 'KC_Width_Mean_16', 'Body_Size_Mean_16', 'Upper_Wick_Mean_16', 'Lower_Wick_Mean_16', 'Range_Mean_16', 'Body_Ratio_Mean_16', 'Upper_Wick_Ratio_Mean_16', 'Lower_Wick_Ratio_Mean_16', 'Doji-OBV_Mean_16', 'Volume-ATR_Mean_16', 'VWAP-ATR_Mean_16', 'RSI-MACD_Mean_16', 'BB-KC_Mean_16', 'Ichimoku_Overlap_Mean_16', 'LOW-HIGH_EMA_Mean_16',
                'Stochastic-RSI_Mean_48', 'BTC-ETC_Diff_Mean_48', 'Price_Diff_Mean_48', 'Returns_Mean_48', 'Volatility_Mean_48', 'Volume_Change_Mean_48', 'OBV_Mean_48', 'CMF_Mean_48', 'VWAP_Mean_48', 'RSI_Mean_48', 'Fast_%K_Mean_48', 'Fast_%D_Mean_48', 'WILLR_Mean_48', 'CCI_Mean_48', 'ATR_Mean_48', 'ADX_Mean_48', 'Momentum_Mean_48', 'TSI_Mean_48', 'BB_Width_Mean_48', 'MACD_Mean_48', 'MACDSignal_Mean_48', 'MACDHist_Mean_48', 'KC_Width_Mean_48', 'Body_Size_Mean_48', 'Upper_Wick_Mean_48', 'Lower_Wick_Mean_48', 'Range_Mean_48', 'Body_Ratio_Mean_48', 'Upper_Wick_Ratio_Mean_48', 'Lower_Wick_Ratio_Mean_48', 'Doji-OBV_Mean_48', 'Volume-ATR_Mean_48', 'VWAP-ATR_Mean_48', 'RSI-MACD_Mean_48', 'BB-KC_Mean_48', 'Ichimoku_Overlap_Mean_48', 'LOW-HIGH_EMA_Mean_48',
                'Stochastic-RSI_Mean_96', 'BTC-ETC_Diff_Mean_96', 'Price_Diff_Mean_96', 'Returns_Mean_96', 'Volatility_Mean_96', 'Volume_Change_Mean_96', 'OBV_Mean_96', 'CMF_Mean_96', 'VWAP_Mean_96', 'RSI_Mean_96', 'Fast_%K_Mean_96', 'Fast_%D_Mean_96', 'WILLR_Mean_96', 'CCI_Mean_96', 'ATR_Mean_96', 'ADX_Mean_96', 'Momentum_Mean_96', 'TSI_Mean_96', 'BB_Width_Mean_96', 'MACD_Mean_96', 'MACDSignal_Mean_96', 'MACDHist_Mean_96', 'KC_Width_Mean_96', 'Body_Size_Mean_96', 'Upper_Wick_Mean_96', 'Lower_Wick_Mean_96', 'Range_Mean_96', 'Body_Ratio_Mean_96', 'Upper_Wick_Ratio_Mean_96', 'Lower_Wick_Ratio_Mean_96', 'Doji-OBV_Mean_96', 'Volume-ATR_Mean_96', 'VWAP-ATR_Mean_96', 'RSI-MACD_Mean_96', 'BB-KC_Mean_96', 'Ichimoku_Overlap_Mean_96', 'LOW-HIGH_EMA_Mean_96',
                'Stochastic-RSI_Mean_384', 'BTC-ETC_Diff_Mean_384', 'Price_Diff_Mean_384', 'Returns_Mean_384', 'Volatility_Mean_384', 'Volume_Change_Mean_384', 'OBV_Mean_384', 'CMF_Mean_384', 'VWAP_Mean_384', 'RSI_Mean_384', 'Fast_%K_Mean_384', 'Fast_%D_Mean_384', 'WILLR_Mean_384', 'CCI_Mean_384', 'ATR_Mean_384', 'ADX_Mean_384', 'Momentum_Mean_384', 'TSI_Mean_384', 'BB_Width_Mean_384', 'MACD_Mean_384', 'MACDSignal_Mean_384', 'MACDHist_Mean_384', 'KC_Width_Mean_384', 'Body_Size_Mean_384', 'Upper_Wick_Mean_384', 'Lower_Wick_Mean_384', 'Range_Mean_384', 'Body_Ratio_Mean_384', 'Upper_Wick_Ratio_Mean_384', 'Lower_Wick_Ratio_Mean_384', 'Doji-OBV_Mean_384', 'Volume-ATR_Mean_384', 'VWAP-ATR_Mean_384', 'RSI-MACD_Mean_384', 'BB-KC_Mean_384', 'Ichimoku_Overlap_Mean_384', 'LOW-HIGH_EMA_Mean_384',
                'Stochastic-RSI_Mean_1152', 'BTC-ETC_Diff_Mean_1152', 'Price_Diff_Mean_1152', 'Returns_Mean_1152', 'Volatility_Mean_1152', 'Volume_Change_Mean_1152', 'OBV_Mean_1152', 'CMF_Mean_1152', 'VWAP_Mean_1152', 'RSI_Mean_1152', 'Fast_%K_Mean_1152', 'Fast_%D_Mean_1152', 'WILLR_Mean_1152', 'CCI_Mean_1152', 'ATR_Mean_1152', 'ADX_Mean_1152', 'Momentum_Mean_1152', 'TSI_Mean_1152', 'BB_Width_Mean_1152', 'MACD_Mean_1152', 'MACDSignal_Mean_1152', 'MACDHist_Mean_1152', 'KC_Width_Mean_1152', 'Body_Size_Mean_1152', 'Upper_Wick_Mean_1152', 'Lower_Wick_Mean_1152', 'Range_Mean_1152', 'Body_Ratio_Mean_1152', 'Upper_Wick_Ratio_Mean_1152', 'Lower_Wick_Ratio_Mean_1152', 'Doji-OBV_Mean_1152', 'Volume-ATR_Mean_1152', 'VWAP-ATR_Mean_1152', 'RSI-MACD_Mean_1152', 'BB-KC_Mean_1152', 'Ichimoku_Overlap_Mean_1152', 'LOW-HIGH_EMA_Mean_1152',
                'RSI_PC1_Mean_4',  'RSI_PC2_Mean_4',  'RSI_PC3_Mean_4',  'ATR_PC1_Mean_4',  'ATR_PC2_Mean_4',  'ATR_PC3_Mean_4',  'ADX_PC1_Mean_4',  'ADX_PC2_Mean_4',  'ADX_PC3_Mean_4',  'ADX_PC4_Mean_4',  'ADX_PC5_Mean_4',
                'RSI_PC1_Mean_16',  'RSI_PC2_Mean_16',  'RSI_PC3_Mean_16',  'ATR_PC1_Mean_16',  'ATR_PC2_Mean_16',  'ATR_PC3_Mean_16',  'ADX_PC1_Mean_16',  'ADX_PC2_Mean_16',  'ADX_PC3_Mean_16',  'ADX_PC4_Mean_16',  'ADX_PC5_Mean_16',
                'RSI_PC1_Mean_48',  'RSI_PC2_Mean_48',  'RSI_PC3_Mean_48',  'ATR_PC1_Mean_48',  'ATR_PC2_Mean_48',  'ATR_PC3_Mean_48',  'ADX_PC1_Mean_48',  'ADX_PC2_Mean_48',  'ADX_PC3_Mean_48',  'ADX_PC4_Mean_48',  'ADX_PC5_Mean_48',
                'RSI_PC1_Mean_96',  'RSI_PC2_Mean_96',  'RSI_PC3_Mean_96',  'ATR_PC1_Mean_96',  'ATR_PC2_Mean_96',  'ATR_PC3_Mean_96',  'ADX_PC1_Mean_96',  'ADX_PC2_Mean_96',  'ADX_PC3_Mean_96',  'ADX_PC4_Mean_96',  'ADX_PC5_Mean_96',
                'RSI_PC1_Mean_384',  'RSI_PC2_Mean_384',  'RSI_PC3_Mean_384',  'ATR_PC1_Mean_384',  'ATR_PC2_Mean_384',  'ATR_PC3_Mean_384',  'ADX_PC1_Mean_384',  'ADX_PC2_Mean_384',  'ADX_PC3_Mean_384',  'ADX_PC4_Mean_384',  'ADX_PC5_Mean_384',
                'RSI_PC1_Mean_1152',  'RSI_PC2_Mean_1152',  'RSI_PC3_Mean_1152',  'ATR_PC1_Mean_1152',  'ATR_PC2_Mean_1152',  'ATR_PC3_Mean_1152',  'ADX_PC1_Mean_1152',  'ADX_PC2_Mean_1152',  'ADX_PC3_Mean_1152',  'ADX_PC4_Mean_1152',  'ADX_PC5_Mean_1152'
                ]

# 'RSI_Mean_4', 'RSI_Mean_16', 'TSI_Mean_16', 'Volume_Change_Mean_4', 'Price_Diff_Mean_4', 'TSI_Mean_4', 'Returns_Mean_4', 'Lower_Wick_Ratio_Mean_4',
#                 'BB-KC_Mean_96', 'Returns_Mean_16', 'Price_Diff_Mean_16', 'Body_Ratio_Mean_4', 'Label_Mean_16', 'Price_Diff_Mean_96', 'CCI_Mean_4', 'MACDHist_Mean_96', 'CCI_Mean_96', 'Doji-RSI_Mean_4', 'Belt_Hold-ATR_Mean_48', 'Label_Mean_96',
#                 'VWAP_Mean_48', 'Long_Doji-RSI_Mean_4', 'Short_Candle-RSI_Mean_4', 'Aroon_Mean_4', 'CCI_Mean_16', 'Long_Candle-RSI_Mean_4', 'ADX_Mean_4', 'Aroon_Mean_16', 'Volume_Change_Mean_16', 'MACDSignal_Mean_4',
#                 'Stochastic-RSI_Mean_48', 'MACDSignal_Mean_16', 'Closing_Marubozu-ATR_Mean_4', 'Volume-ATR_Mean_4', 'Upper_Wick_Ratio_Mean_4', 'Upper_Wick_Mean_4', 'Returns_Mean_48', 'Marubozu-OBV_Mean_16',
#                 'Dragonfly_Doji-RSI_Mean_48', 'Ichimoku_Overlap_Mean_48', 'Lower_Wick_Ratio_Mean_48', 'RSI_Mean_48', 'Price_Diff_Mean_48', 'Fast_%K_Mean_4', 'Aroon_Mean_96', 'Hammer_Mean_16', 'Body_Ratio_Mean_16',
#                 'Engulfing_Mean_4', 'VWAP_Mean_4', 'Belt_Hold-ATR_Mean_96', 'Returns_Mean_96', 'Marubozu-RSI_Mean_48', 'Hikkake_Mean_16', 'LOW-HIGH_EMA_Mean_4', 'Chikou_Span_Dev_48', 'Long_Doji-RSI_Dev_4',
#                 'Doji-RSI_Dev_48', 'Long_Candle-RSI_Dev_48', 'Stochastic-RSI_Dev_96', 'Closing_Marubozu-RSI_Dev_48', 'ADX_Dev_4', 'Stochastic-RSI_Dev_4', 'Lower_Wick_Ratio_Dev_4', 'Long_Candle_Dev_384',
#                 'ADX_Dev_96', 'High_Dev_4', 'TSI_Dev_48', 'Engulfing_Dev_48', 'VWAP_Dev_48', 'Volume-ATR_Dev_4', 'OBV_Dev_4', 'BB_Upper_Dev_4', 'TSI_Dev_4', 'BB_Lower_Dev_4', 'Fast_%D_Dev_4', 'KC_Lower_Dev_4',
#                 'Volume_Dev_4', 'Long_Doji-RSI_Dev_16', 'Senkou_Span_A_Dev_4', 'Senkou_Span_B_Dev_16', 'CCI_Dev_4', 'Aroon_Dev_96', 'Stochastic-RSI_Dev_48', 'Lower_Wick_Ratio_Dev_48', 'BB-KC_Dev_16', 'Upper_Wick_Dev_4',
#                 'Dragonfly_Doji-OBV_Dev_48', 'Belt_Hold-RSI_Dev_48', 'Kijun_Sen_Dev_16', 'Closing_Marubozu-RSI_Dev_4', 'Upper_Wick_Ratio_Dev_4', 'Short_Candle-RSI_Dev_96', 'Long_Doji-RSI_Dev_48', 'Long_Candle-RSI_Dev_16', 'Closing_Marubozu-RSI_Dev_16',
#                 'Fast_%K_Dev_48', 'MACDSignal_Dev_4', 'CCI_Dev_48', 'Belt_Hold-RSI_Dev_4', 'RSI_Dev_48', 'Short_Candle-OBV_Dev_16', 'VWAP-ATR_Dev_4', 'Long_Candle-RSI_Dev_384', 'Closing_Marubozu-RSI_Dev_96', 'BTC-ETC_Diff_Dev_4', 'Doji-RSI_Dev_4',
#                 'Chikou_Span_Drawdown_4', 'KC_Upper_Drawdown_4', 'Volatility_Drawdown_4', 'CMF_Drawdown_4', 'ADX_Drawdown_4', 'ATR_Drawdown_4', 'S2_Drawdown_4', 'KC_Upper_Drawdown_16', 'Open_Drawdown_4', 'Fast_%D_Drawdown_4', 'ADX_Drawdown_16', 'BB_Lower_Drawdown_4',
#                 'RSI-MACD_Drawdown_4', 'Chikou_Span_Drawdown_16', 'BB_Upper_Drawdown_4', 'Volatility_Drawdown_16', 'R2_Drawdown_4', 'RSI_Drawdown_4', 'BB_Upper_Drawdown_16', 'BB-KC_Drawdown_4', 'BB_Lower_Drawdown_16', 'CMF_Drawdown_16', 'S1_Drawdown_4',
#                 'Low_Drawdown_4', 'VWAP-ATR_Drawdown_16', 'BB_Middle_Drawdown_4', 'R2_Drawdown_16', 'Senkou_Span_B_Drawdown_4', 'High_Drawdown_4', 'Chikou_Span_Drawdown_48'
model = load_xgboost("xgboost_pipeline.pkl")
# data_test = get_historical_data( 1739645100000, 1744825500000)
# pca_training_df = get_historical_data(1735707600000,  	1738386000000)
# data_test = load_and_preprocess_data("", False, data=data_test, pca_training_data=pca_training_df)
#
# xg_data_test = convert_data_to_windows(data_test[xg_features], window_size)
# data_test = data_test.drop(index=data_test.index[:window_size])
# xg_data_test = xg_data_test.reset_index(drop=True)
# data_test = data_test.reset_index(drop=True)
# xg_data_test = pd.concat([xg_data_test, data_test[agg_features]], axis=1)
# xg_data_test.dropna(inplace=True)
# xg_data_test.drop(['Label'], axis=1, inplace=True)
# xg_data_test.drop(['Weights'], axis=1, inplace=True)
#
# walk_forward_trading(model, xg_data_test)
continuous_sim(model, xg_features, agg_features)

#walk_forward_trading(model, xg_data_test)


# #
# recent_states = []
# recent_df_current = pd.DataFrame(columns=recent_df[hmm_features].columns)
# recent_count = 0
# for index, row in recent_df[hmm_features].iterrows():
#     recent_count += 1
#     recent_df_current = pd.concat([recent_df_current, pd.DataFrame([row.to_dict()])], ignore_index=True)
#     recent_states.append(predict_states(hmm_model, recent_df_current, hmm_features, scaler)[-1])
#     #keeps a window of 500 to improve computational efficiency
#     if recent_count >= 1000:
#         recent_df_current = recent_df_current.drop(recent_df_current.index[0])
#
# recent_states = list(zip(*recent_states))
#
# for i in range(hmm_comps):
#     xg_features.append(f'State{i}')
#     recent_df[f'State{i}'] = recent_states[i]
#
# #Need this to make recent_df compatible with convert_data_to_windows method
# recent_df['Label'] = 0
# print(recent_df['Close'])
# recent_df['Weights'] = 0
# recent_df_windowed = convert_data_to_windows(recent_df[xg_features], window_size)
#
# recent_df = recent_df.drop(index=recent_df.index[:window_size])
# recent_df = recent_df.reset_index(drop=True)
# recent_df_windowed = recent_df_windowed.reset_index(drop=True)
# recent_df = pd.concat([recent_df_windowed, recent_df[agg_features]], axis=1)
# xg_labels_test = recent_df['Label'].tolist()
# print(recent_df[['Label', 'Close_t-1']])
# recent_df.drop(['Label'], axis=1, inplace=True)
# recent_df.drop(['Weights'], axis=1, inplace=True)
#
# xg_labels_pred = xgboost_model.predict(recent_df)
# xg_labels_pred = xg_labels_pred.tolist()
# recent_df['Pred_Label'] = xg_labels_pred
#
# count0 = 0
# count1 = 0
# for i in range(len(xg_labels_pred)):
#     if xg_labels_pred[i] == 0 and xg_labels_test[i] == 0:
#         count0 += 1
#     if xg_labels_pred[i] == 1 and xg_labels_test[i] == 1:
#         count1 += 1
#
# accuracy0 = count0 / xg_labels_test.count(0)
# accuracy1 = count1 / xg_labels_test.count(1)
# ratio0 = xg_labels_test.count(0) / len(xg_labels_test)
# ratio1 = xg_labels_test.count(1) / len(xg_labels_test)
# predratio0 = xg_labels_test.count(0) / xg_labels_pred.count(0)
# predratio1 = xg_labels_test.count(1) / xg_labels_pred.count(1)
# print(f"Accuracy of sells: {accuracy0}")
# print(f"Accuracy of buys: {accuracy1}")
# print(f"Combined accuracy: {accuracy0 + accuracy1}")
# print(f"Ratio of sells: {ratio0}")
# print(f"Ratio of buys: {ratio1}")
# print(f"Ratio of real 0 labels to predicted 0 labels: {predratio0}")
# print(f"Ratio of real 1 labels to predicted 1 labels: {predratio1}")
# print(xg_labels_test.count(0))
# print(xg_labels_pred.count(0))
# print(xg_labels_test.count(1))
# print(xg_labels_pred.count(1))

#
# probabilities = xgboost_model.predict_proba(recent_df).tolist()
# probability_0 = []
# probability_1 = []
# for probability in probabilities:
#     probability_0.append(probability[0])
#     probability_1.append(probability[1])
#
# labels = xgboost_model.predict(recent_df)
# for i in range(len(labels)):
#     print(f"label: {labels[i]}, price: {closes[i]}")
#
# print(f"percentile of sells: {np.percentile(probability_0, 53)}")
# print(f"percentile of buys: {np.percentile(probability_1, 53)}")
# closes = recent_df['Close_t-1'].tolist()
# print(recent_df[['Pred_Label', 'Close_t-1']])
# trading_simulation(xg_labels_pred, closes, starting_money=1500, spend_percentage=1)
# notify()
#while True:
#     start_time = time.time()
#
#     recent_df = calculate_indicators(recent_df)
#     recent_df = recent_df.dropna()
#     recent_df['State'] = predict_states(hmm_model, recent_df, hmm_features, scaler)
#
#     #Need this to make recent_df compatible with convert_data_to_windows method
#     recent_df['Label'] = 0
#     recent_df_windowed = convert_data_to_windows(recent_df[xg_features], window_size)
#
#     recent_df = recent_df.drop(index=recent_df.index[:window_size])
#     recent_df = recent_df.reset_index(drop=True)
#     recent_df_windowed = recent_df_windowed.reset_index(drop=True)
#     recent_df = pd.concat([recent_df_windowed, recent_df[agg_features]], axis=1)
#     print(recent_df)
#     recent_df.drop(['Label'], axis=1, inplace=True)
#
#     labels_recent_pred = xgboost_model.predict(recent_df)
#     labels_recent_pred = labels_recent_pred.tolist()
#     recent_df['Label'] = labels_recent_pred
#     recent_df['PriceDiff'] = recent_df['Close_t-1'].diff()
#
#     trading_df = recent_df[['Close_t-1', 'Label']]
#     print(trading_df)
#
#     time.sleep(180)
#     recent_df = get_historical_data(54)